# ç¬¬ä¸€ç« ï¼šé¡¹ç›®æ¦‚è¿°ä¸å¿«é€Ÿä¸Šæ‰‹

> æœ¬ç« å°†ä»‹ç» Nano-vLLM é¡¹ç›®çš„èƒŒæ™¯ã€è®¾è®¡ç›®æ ‡ï¼Œå¹¶é€šè¿‡å¿«é€Ÿä¸Šæ‰‹ç¤ºä¾‹è®©ä½ äº†è§£åŸºæœ¬ä½¿ç”¨æ–¹æ³•ã€‚

## 1.1 é¡¹ç›®ç®€ä»‹

### ä»€ä¹ˆæ˜¯ Nano-vLLMï¼Ÿ

Nano-vLLM æ˜¯ä¸€ä¸ª**è½»é‡çº§çš„ vLLM å®ç°**ï¼Œä»…ç”¨çº¦ 1200 è¡Œ Python ä»£ç å®ç°äº†ä¸ vLLM ç›¸å½“çš„æ¨ç†æ€§èƒ½ã€‚å®ƒçš„è®¾è®¡ç›®æ ‡æ˜¯ï¼š

- ğŸš€ **é«˜æ€§èƒ½**ï¼šä¸ vLLM ç›¸å½“çš„æ¨ç†é€Ÿåº¦
- ğŸ“– **å¯è¯»æ€§**ï¼šç®€æ´æ¸…æ™°çš„ä»£ç å®ç°
- âš¡ **ä¼˜åŒ–ä¸°å¯Œ**ï¼šPrefix Cachingã€å¼ é‡å¹¶è¡Œã€CUDA Graph ç­‰

### æ€§èƒ½å¯¹æ¯”

| æ¨ç†å¼•æ“ | è¾“å‡º Token æ•° | è€—æ—¶ (s) | ååé‡ (tokens/s) |
|:---------|:-------------|:---------|:------------------|
| vLLM     | 133,966      | 98.37    | 1361.84           |
| Nano-vLLM| 133,966      | 93.41    | **1434.13**       |

> æµ‹è¯•ç¯å¢ƒï¼šRTX 4070 Laptop (8GB)ï¼ŒQwen3-0.6B æ¨¡å‹

---

## 1.2 é¡¹ç›®ç»“æ„

```mermaid
graph TB
    subgraph "å…¥å£å±‚"
        A[nanovllm/__init__.py]
        B[nanovllm/llm.py]
    end
    
    subgraph "é…ç½®å±‚"
        C[config.py]
        D[sampling_params.py]
    end
    
    subgraph "å¼•æ“å±‚ engine/"
        E[llm_engine.py]
        F[scheduler.py]
        G[block_manager.py]
        H[sequence.py]
        I[model_runner.py]
    end
    
    subgraph "ç¥ç»ç½‘ç»œå±‚ layers/"
        J[linear.py]
        K[attention.py]
        L[rotary_embedding.py]
        M[layernorm.py]
        N[activation.py]
        O[embed_head.py]
        P[sampler.py]
    end
    
    subgraph "æ¨¡å‹å±‚ models/"
        Q[qwen3.py]
    end
    
    subgraph "å·¥å…·å±‚ utils/"
        R[context.py]
        S[loader.py]
    end
    
    A --> B
    B --> E
    E --> F
    E --> I
    F --> G
    F --> H
    I --> Q
    Q --> J
    Q --> K
    Q --> L
    Q --> M
    Q --> N
    Q --> O
    I --> P
    I --> R
    I --> S
```

---

## 1.3 æºç åˆ†æï¼šåŒ…å…¥å£

### `nanovllm/__init__.py` æºç 

```python
from nanovllm.llm import LLM
from nanovllm.sampling_params import SamplingParams
```

**é€è¡Œè§£æ**ï¼š

| è¡Œå· | ä»£ç  | è¯´æ˜ |
|:---:|:---|:---|
| 1 | `from nanovllm.llm import LLM` | å¯¼å…¥ LLM ç±»ï¼Œè¿™æ˜¯ç”¨æˆ·ä½¿ç”¨çš„ä¸»è¦æ¥å£ |
| 2 | `from nanovllm.sampling_params import SamplingParams` | å¯¼å…¥é‡‡æ ·å‚æ•°ç±»ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆè¡Œä¸º |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šéµå¾ªã€Œæœ€å°æš´éœ²åŸåˆ™ã€ï¼Œ`__init__.py` åªå¯¼å‡ºç”¨æˆ·çœŸæ­£éœ€è¦çš„ä¸¤ä¸ªç±»ï¼Œéšè—å†…éƒ¨å®ç°ç»†èŠ‚ã€‚è¿™ç§è®¾è®¡è®© API ä¿æŒç®€æ´ï¼Œç”¨æˆ·æ— éœ€äº†è§£åº•å±‚çš„ `LLMEngine`ã€`Scheduler` ç­‰å¤æ‚ç»„ä»¶ã€‚

### `nanovllm/llm.py` æºç 

```python
from nanovllm.engine.llm_engine import LLMEngine


class LLM(LLMEngine):
    pass
```

**é€è¡Œè§£æ**ï¼š

| è¡Œå· | ä»£ç  | è¯´æ˜ |
|:---:|:---|:---|
| 1 | `from nanovllm.engine.llm_engine import LLMEngine` | å¯¼å…¥æ¨ç†å¼•æ“åŸºç±» |
| 4-5 | `class LLM(LLMEngine): pass` | LLM ç±»ç›´æ¥ç»§æ‰¿ LLMEngineï¼Œä¸æ·»åŠ é¢å¤–åŠŸèƒ½ |

**è®¾è®¡æ€è€ƒ**ï¼š

ä¸ºä»€ä¹ˆè¦åˆ›å»ºä¸€ä¸ªç©ºçš„ `LLM` ç±»ç»§æ‰¿ `LLMEngine`ï¼Ÿ

1. **API å…¼å®¹æ€§**ï¼šä¸ vLLM ä¿æŒç›¸åŒçš„ç±»å `LLM`
2. **æ‰©å±•é¢„ç•™**ï¼šæœªæ¥å¯ä»¥åœ¨ `LLM` ç±»ä¸­æ·»åŠ é«˜çº§åŠŸèƒ½
3. **èŒè´£åˆ†ç¦»**ï¼š`LLMEngine` ä¸“æ³¨åº•å±‚å®ç°ï¼Œ`LLM` æä¾›ç”¨æˆ·æ¥å£

---

## 1.4 æºç åˆ†æï¼šä½¿ç”¨ç¤ºä¾‹

### `example.py` å®Œæ•´æºç 

```python
import os
from nanovllm import LLM, SamplingParams
from transformers import AutoTokenizer


def main():
    path = os.path.expanduser("~/huggingface/Qwen3-0.6B/")
    tokenizer = AutoTokenizer.from_pretrained(path)
    llm = LLM(path, enforce_eager=True, tensor_parallel_size=1)

    sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
    prompts = [
        "introduce yourself",
        "list all prime numbers within 100",
    ]
    prompts = [
        tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}],
            tokenize=False,
            add_generation_prompt=True,
        )
        for prompt in prompts
    ]
    outputs = llm.generate(prompts, sampling_params)

    for prompt, output in zip(prompts, outputs):
        print("\n")
        print(f"Prompt: {prompt!r}")
        print(f"Completion: {output['text']!r}")


if __name__ == "__main__":
    main()
```

### é€è¡Œè¯¦è§£

#### ç¬¬ 1-3 è¡Œï¼šå¯¼å…¥ä¾èµ–

```python
import os
from nanovllm import LLM, SamplingParams
from transformers import AutoTokenizer
```

| æ¨¡å— | ç”¨é€” |
|:---|:---|
| `os` | å¤„ç†æ–‡ä»¶è·¯å¾„ï¼ˆå±•å¼€ `~`ï¼‰ |
| `LLM` | æ¨ç†å¼•æ“ä¸»ç±» |
| `SamplingParams` | é‡‡æ ·å‚æ•°æ§åˆ¶ |
| `AutoTokenizer` | HuggingFace åˆ†è¯å™¨ |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šNano-vLLM é€‰æ‹©å¤ç”¨ HuggingFace çš„ `AutoTokenizer` è€Œéè‡ªå·±å®ç°åˆ†è¯å™¨ï¼Œè¿™ä½“ç°äº†ã€Œä¸é‡å¤é€ è½®å­ã€çš„å·¥ç¨‹ç†å¿µâ€”â€”ä¸“æ³¨äºæ¨ç†å¼•æ“æ ¸å¿ƒé€»è¾‘ï¼Œå…¶ä»–æˆç†Ÿç»„ä»¶ç›´æ¥å¤ç”¨ã€‚

#### ç¬¬ 7-9 è¡Œï¼šåˆå§‹åŒ–

```python
path = os.path.expanduser("~/huggingface/Qwen3-0.6B/")
tokenizer = AutoTokenizer.from_pretrained(path)
llm = LLM(path, enforce_eager=True, tensor_parallel_size=1)
```

**å…³é”®å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | å€¼ | è¯´æ˜ |
|:---|:---|:---|
| `path` | æ¨¡å‹ç›®å½• | åŒ…å«æ¨¡å‹æƒé‡å’Œé…ç½®çš„ç›®å½• |
| `enforce_eager` | `True` | ç¦ç”¨ CUDA Graphï¼Œä½¿ç”¨å³æ—¶æ‰§è¡Œæ¨¡å¼ |
| `tensor_parallel_size` | `1` | å• GPU è¿è¡Œï¼Œä¸ä½¿ç”¨å¼ é‡å¹¶è¡Œ |

> ğŸ’¡ `enforce_eager=True` é€‚åˆè°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ä¸º `False` ä»¥å¯ç”¨ CUDA Graph ä¼˜åŒ–

#### ç¬¬ 11-22 è¡Œï¼šå‡†å¤‡è¾“å…¥

```python
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = [
    "introduce yourself",
    "list all prime numbers within 100",
]
prompts = [
    tokenizer.apply_chat_template(
        [{"role": "user", "content": prompt}],
        tokenize=False,
        add_generation_prompt=True,
    )
    for prompt in prompts
]
```

**é‡‡æ ·å‚æ•°**ï¼š

| å‚æ•° | å€¼ | è¯´æ˜ |
|:---|:---|:---|
| `temperature` | `0.6` | æ¸©åº¦ç³»æ•°ï¼Œè¶Šä½è¶Šç¡®å®šæ€§ |
| `max_tokens` | `256` | æœ€å¤§ç”Ÿæˆ token æ•° |

**apply_chat_template å¤„ç†**ï¼š

å°†ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„å¯¹è¯æ ¼å¼ï¼Œä¾‹å¦‚ï¼š
```
<|im_start|>user
introduce yourself<|im_end|>
<|im_start|>assistant
```

#### ç¬¬ 23 è¡Œï¼šæ‰§è¡Œæ¨ç†

```python
outputs = llm.generate(prompts, sampling_params)
```

`generate` æ–¹æ³•æ‰§è¡Œå®Œæ•´çš„æ¨ç†æµç¨‹ï¼š
1. å°† prompt æ·»åŠ åˆ°è¯·æ±‚é˜Ÿåˆ—
2. è°ƒåº¦å™¨åˆ†é…èµ„æº
3. æ¨¡å‹è¿è¡Œå™¨æ‰§è¡Œæ¨ç†
4. è¿”å›ç”Ÿæˆç»“æœ

**è¿”å›æ ¼å¼**ï¼š
```python
outputs[0] = {
    "text": "ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹",
    "token_ids": [12, 345, 67, ...]  # token ID åˆ—è¡¨
}
```

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šè¿”å›ç»“æ„åŒæ—¶åŒ…å«è§£ç åçš„æ–‡æœ¬å’ŒåŸå§‹ token IDsï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚â€”â€”æ–‡æœ¬ç”¨äºç›´æ¥å±•ç¤ºï¼Œtoken IDs ç”¨äºåç»­å¤„ç†ï¼ˆå¦‚è®¡ç®—å›°æƒ‘åº¦ã€token çº§åˆ«åˆ†æç­‰ï¼‰ã€‚

#### ç¬¬ 25-29 è¡Œï¼šè¾“å‡ºç»“æœ

```python
for prompt, output in zip(prompts, outputs):
    print("\n")
    print(f"Prompt: {prompt!r}")
    print(f"Completion: {output['text']!r}")
```

éå†è¾“å…¥å’Œè¾“å‡ºï¼Œæ‰“å°ç»“æœã€‚`!r` ä½¿ç”¨ `repr()` æ ¼å¼åŒ–ï¼Œæ˜¾ç¤ºè½¬ä¹‰å­—ç¬¦ã€‚

---

## 1.5 å¿«é€Ÿä¸Šæ‰‹

### å®‰è£…æ–¹æ³•

```bash
pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
```

### ä¸‹è½½æ¨¡å‹

```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

### æœ€ç®€ç¤ºä¾‹

```python
from nanovllm import LLM, SamplingParams

# åˆå§‹åŒ–
llm = LLM("/path/to/Qwen3-0.6B", enforce_eager=True)

# é…ç½®é‡‡æ ·å‚æ•°
params = SamplingParams(temperature=0.6, max_tokens=256)

# ç”Ÿæˆ
outputs = llm.generate(["Hello, Nano-vLLM."], params)
print(outputs[0]["text"])
```

---

## 1.6 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **é¡¹ç›®å®šä½**ï¼šNano-vLLM æ˜¯è½»é‡çº§ã€é«˜æ€§èƒ½çš„ vLLM å®ç°
2. **é¡¹ç›®ç»“æ„**ï¼šå…¥å£å±‚ã€é…ç½®å±‚ã€å¼•æ“å±‚ã€ç¥ç»ç½‘ç»œå±‚ã€æ¨¡å‹å±‚ã€å·¥å…·å±‚
3. **åŒ…å…¥å£è®¾è®¡**ï¼šç®€æ´çš„ `__init__.py` å’Œç»§æ‰¿è®¾è®¡
4. **ä½¿ç”¨æ–¹æ³•**ï¼šé€šè¿‡ `example.py` äº†è§£åŸºæœ¬ä½¿ç”¨æµç¨‹

---

**ä¸‹ä¸€ç« ** â†’ [02 æ ¸å¿ƒæ¶æ„æ€»è§ˆ](02_core_architecture.md)
