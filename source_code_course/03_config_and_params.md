# ç¬¬ä¸‰ç« ï¼šé…ç½®ä¸é‡‡æ ·å‚æ•°

> æœ¬ç« å°†é€è¡Œåˆ†æ `config.py` å’Œ `sampling_params.py`ï¼Œç†è§£ Nano-vLLM çš„é…ç½®ç³»ç»Ÿã€‚

## 3.1 Config é…ç½®ç±»

### 3.1.1 å®Œæ•´æºç 

```python
import os
from dataclasses import dataclass
from transformers import AutoConfig


@dataclass
class Config:
    model: str
    max_num_batched_tokens: int = 16384
    max_num_seqs: int = 512
    max_model_len: int = 4096
    gpu_memory_utilization: float = 0.9
    tensor_parallel_size: int = 1
    enforce_eager: bool = False
    hf_config: AutoConfig | None = None
    eos: int = -1
    kvcache_block_size: int = 256
    num_kvcache_blocks: int = -1

    def __post_init__(self):
        assert os.path.isdir(self.model)
        assert self.kvcache_block_size % 256 == 0
        assert 1 <= self.tensor_parallel_size <= 8
        self.hf_config = AutoConfig.from_pretrained(self.model)
        self.max_model_len = min(self.max_model_len, self.hf_config.max_position_embeddings)
        assert self.max_num_batched_tokens >= self.max_model_len
```

### 3.1.2 é€è¡Œåˆ†æ

#### å¯¼å…¥éƒ¨åˆ†ï¼ˆç¬¬ 1-3 è¡Œï¼‰

```python
import os
from dataclasses import dataclass
from transformers import AutoConfig
```

| å¯¼å…¥ | ç”¨é€” |
|:---|:---|
| `os` | æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨ |
| `dataclass` | æ•°æ®ç±»è£…é¥°å™¨ï¼Œè‡ªåŠ¨ç”Ÿæˆ `__init__` ç­‰æ–¹æ³• |
| `AutoConfig` | åŠ è½½ HuggingFace æ¨¡å‹é…ç½® |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šä½¿ç”¨ `dataclass` è€Œéæ™®é€šç±»ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆ `__init__`ã€`__repr__` ç­‰æ–¹æ³•ï¼Œå‡å°‘æ ·æ¿ä»£ç ï¼ŒåŒæ—¶ä¿æŒä»£ç ç®€æ´å¯è¯»ã€‚

#### ç±»å®šä¹‰ä¸å­—æ®µï¼ˆç¬¬ 6-18 è¡Œï¼‰

```python
@dataclass
class Config:
    model: str
    max_num_batched_tokens: int = 16384
    max_num_seqs: int = 512
    max_model_len: int = 4096
    gpu_memory_utilization: float = 0.9
    tensor_parallel_size: int = 1
    enforce_eager: bool = False
    hf_config: AutoConfig | None = None
    eos: int = -1
    kvcache_block_size: int = 256
    num_kvcache_blocks: int = -1
```

**å­—æ®µè¯¦è§£**ï¼š

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|:---|:---|:---|:---|
| `model` | `str` | å¿…å¡« | æ¨¡å‹è·¯å¾„ |
| `max_num_batched_tokens` | `int` | 16384 | å•æ‰¹æ¬¡æœ€å¤§ token æ•° |
| `max_num_seqs` | `int` | 512 | æœ€å¤§å¹¶å‘åºåˆ—æ•° |
| `max_model_len` | `int` | 4096 | å•åºåˆ—æœ€å¤§é•¿åº¦ |
| `gpu_memory_utilization` | `float` | 0.9 | GPU æ˜¾å­˜åˆ©ç”¨ç‡ |
| `tensor_parallel_size` | `int` | 1 | å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆGPU æ•°ï¼‰ |
| `enforce_eager` | `bool` | False | æ˜¯å¦ç¦ç”¨ CUDA Graph |
| `hf_config` | `AutoConfig` | None | HuggingFace æ¨¡å‹é…ç½® |
| `eos` | `int` | -1 | ç»“æŸ token ID |
| `kvcache_block_size` | `int` | 256 | KV Cache å—å¤§å° |
| `num_kvcache_blocks` | `int` | -1 | KV Cache æ€»å—æ•°ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰ |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šé…ç½®å­—æ®µåˆ†ä¸ºä¸¤ç±»â€”â€”ç”¨æˆ·å¯æ§ï¼ˆå¦‚ `max_num_seqs`ï¼‰å’Œè‡ªåŠ¨è®¡ç®—ï¼ˆå¦‚ `num_kvcache_blocks`ï¼‰ã€‚è‡ªåŠ¨è®¡ç®—çš„å­—æ®µç”¨ `-1` ä½œä¸ºå“¨å…µå€¼ï¼Œè¡¨ç¤ºã€Œå¾…è¿è¡Œæ—¶ç¡®å®šã€ï¼Œé¿å…ç”¨æˆ·éœ€è¦æ‰‹åŠ¨è®¡ç®—å¤æ‚çš„å†…å­˜å¸ƒå±€ã€‚

#### ååˆå§‹åŒ–éªŒè¯ï¼ˆç¬¬ 20-26 è¡Œï¼‰

```python
def __post_init__(self):
    assert os.path.isdir(self.model)
    assert self.kvcache_block_size % 256 == 0
    assert 1 <= self.tensor_parallel_size <= 8
    self.hf_config = AutoConfig.from_pretrained(self.model)
    self.max_model_len = min(self.max_model_len, self.hf_config.max_position_embeddings)
    assert self.max_num_batched_tokens >= self.max_model_len
```

**`__post_init__` æ‰§è¡Œæµç¨‹**ï¼š

```mermaid
flowchart TD
    A[å¼€å§‹] --> B{æ¨¡å‹ç›®å½•å­˜åœ¨?}
    B -->|å¦| X1[AssertionError]
    B -->|æ˜¯| C{block_size % 256 == 0?}
    C -->|å¦| X2[AssertionError]
    C -->|æ˜¯| D{1 <= tp_size <= 8?}
    D -->|å¦| X3[AssertionError]
    D -->|æ˜¯| E[åŠ è½½ HuggingFace é…ç½®]
    E --> F[è°ƒæ•´ max_model_len]
    F --> G{batched_tokens >= model_len?}
    G -->|å¦| X4[AssertionError]
    G -->|æ˜¯| H[å®Œæˆ]
```

**å…³é”®éªŒè¯é€»è¾‘**ï¼š

| è¡Œå· | éªŒè¯ | åŸå›  |
|:---:|:---|:---|
| 21 | æ¨¡å‹ç›®å½•å¿…é¡»å­˜åœ¨ | ç¡®ä¿èƒ½åŠ è½½æ¨¡å‹æƒé‡ |
| 22 | block_size å¿…é¡»æ˜¯ 256 çš„å€æ•° | Flash Attention çš„å¯¹é½è¦æ±‚ |
| 23 | å¼ é‡å¹¶è¡Œå¤§å°åœ¨ 1-8 ä¹‹é—´ | å®é™…ç¡¬ä»¶é™åˆ¶ |
| 25 | å– max_model_len å’Œæ¨¡å‹é™åˆ¶çš„è¾ƒå°å€¼ | ä¸èƒ½è¶…è¿‡æ¨¡å‹æ”¯æŒçš„æœ€å¤§ä½ç½® |
| 26 | æ‰¹æ¬¡ token æ•° >= åºåˆ—é•¿åº¦ | è‡³å°‘èƒ½å¤„ç†ä¸€ä¸ªå®Œæ•´åºåˆ— |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼š`__post_init__` å®ç°äº†ã€ŒFail-Fastã€åŸåˆ™â€”â€”åœ¨å¯¹è±¡åˆ›å»ºæ—¶å°±éªŒè¯æ‰€æœ‰çº¦æŸï¼Œè€Œéç­‰åˆ°è¿è¡Œæ—¶æ‰å‘ç°é”™è¯¯ã€‚è¿™æ ·å¯ä»¥ç»™å‡ºæ¸…æ™°çš„é”™è¯¯ä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿå®šä½é…ç½®é—®é¢˜ã€‚

### 3.1.3 é…ç½®å…³ç³»å›¾

```mermaid
graph TB
    subgraph "ç”¨æˆ·è¾“å…¥"
        A[model è·¯å¾„]
        B[max_num_seqs]
        C[tensor_parallel_size]
    end
    
    subgraph "è‡ªåŠ¨è®¡ç®—"
        D[hf_config]
        E[max_model_len è°ƒæ•´]
        F[num_kvcache_blocks]
    end
    
    subgraph "è¿è¡Œæ—¶ä½¿ç”¨"
        G[Scheduler]
        H[ModelRunner]
        I[BlockManager]
    end
    
    A -->|"AutoConfig.from_pretrained"| D
    D -->|"max_position_embeddings"| E
    B --> G
    C --> H
    E --> G
    F --> I
```

---

## 3.2 SamplingParams é‡‡æ ·å‚æ•°

### 3.2.1 å®Œæ•´æºç 

```python
from dataclasses import dataclass


@dataclass
class SamplingParams:
    temperature: float = 1.0
    max_tokens: int = 64
    ignore_eos: bool = False

    def __post_init__(self):
        assert self.temperature > 1e-10, "greedy sampling is not permitted"
```

### 3.2.2 é€è¡Œåˆ†æ

#### å¯¼å…¥éƒ¨åˆ†ï¼ˆç¬¬ 1 è¡Œï¼‰

```python
from dataclasses import dataclass
```

ä½¿ç”¨ dataclass ç®€åŒ–å‚æ•°ç±»çš„å®šä¹‰ã€‚

#### ç±»å®šä¹‰ï¼ˆç¬¬ 4-8 è¡Œï¼‰

```python
@dataclass
class SamplingParams:
    temperature: float = 1.0
    max_tokens: int = 64
    ignore_eos: bool = False
```

**å­—æ®µè¯¦è§£**ï¼š

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|:---|:---|:---|:---|
| `temperature` | `float` | 1.0 | é‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ |
| `max_tokens` | `int` | 64 | æœ€å¤§ç”Ÿæˆ token æ•° |
| `ignore_eos` | `bool` | False | æ˜¯å¦å¿½ç•¥ EOS token |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šåªä¿ç•™ 3 ä¸ªæ ¸å¿ƒå‚æ•°ï¼Œä½“ç°äº†ã€Œå°‘å³æ˜¯å¤šã€çš„è®¾è®¡å“²å­¦ã€‚ç›¸æ¯” vLLM æ”¯æŒçš„ `top_k`ã€`top_p`ã€`frequency_penalty` ç­‰åå‡ ä¸ªå‚æ•°ï¼ŒNano-vLLM åªä¿ç•™æœ€å¸¸ç”¨çš„ï¼Œé™ä½ç”¨æˆ·å­¦ä¹ æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒä»£ç ç®€æ´ã€‚

#### æ¸©åº¦é‡‡æ ·åŸç†

```mermaid
graph LR
    subgraph "æ¸©åº¦å½±å“"
        A["temperature = 0.1<br/>ç¡®å®šæ€§é«˜"] 
        B["temperature = 1.0<br/>æ­£å¸¸éšæœº"]
        C["temperature = 2.0<br/>éšæœºæ€§é«˜"]
    end
    
    subgraph "æ¦‚ç‡åˆ†å¸ƒ"
        D["logits / 0.1<br/>åˆ†å¸ƒå°–é”"]
        E["logits / 1.0<br/>åŸå§‹åˆ†å¸ƒ"]
        F["logits / 2.0<br/>åˆ†å¸ƒå¹³ç¼“"]
    end
    
    A --> D
    B --> E
    C --> F
```

**æ•°å­¦å…¬å¼**ï¼š

$$p_i = \frac{e^{logit_i / T}}{\sum_j e^{logit_j / T}}$$

å…¶ä¸­ $T$ æ˜¯æ¸©åº¦å‚æ•°ï¼š
- $T \to 0$ï¼šè¶‹è¿‘ argmaxï¼ˆè´ªå©ªé‡‡æ ·ï¼‰
- $T = 1$ï¼šæ ‡å‡† softmax
- $T > 1$ï¼šæ›´å‡åŒ€çš„åˆ†å¸ƒ

#### ååˆå§‹åŒ–éªŒè¯ï¼ˆç¬¬ 10-11 è¡Œï¼‰

```python
def __post_init__(self):
    assert self.temperature > 1e-10, "greedy sampling is not permitted"
```

**ä¸ºä»€ä¹ˆç¦æ­¢è´ªå©ªé‡‡æ ·ï¼ˆtemperature â‰ˆ 0ï¼‰ï¼Ÿ**

1. **æ•°å€¼ç¨³å®šæ€§**ï¼šæ¸©åº¦æ¥è¿‘ 0 ä¼šå¯¼è‡´ softmax æ•°å€¼æº¢å‡º
2. **è®¾è®¡é€‰æ‹©**ï¼šNano-vLLM ä¸“æ³¨äºéšæœºé‡‡æ ·åœºæ™¯
3. **ç®€åŒ–å®ç°**ï¼šé¿å…å¤„ç† argmax çš„ç‰¹æ®Šæƒ…å†µ

> ğŸ’¡ å¦‚æœéœ€è¦è´ªå©ªé‡‡æ ·ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ªå¾ˆå°çš„æ¸©åº¦å€¼ï¼ˆå¦‚ 0.01ï¼‰

---

## 3.3 é…ç½®åœ¨ç³»ç»Ÿä¸­çš„æµåŠ¨

### 3.3.1 Config çš„ä¼ é€’è·¯å¾„

```mermaid
flowchart LR
    A[ç”¨æˆ·åˆ›å»º LLM] -->|"Config"| B[LLMEngine.__init__]
    B -->|"config"| C[Scheduler]
    B -->|"config"| D[ModelRunner]
    C -->|"config"| E[BlockManager]
    D -->|"config"| F[KV Cache åˆ†é…]
    D -->|"config"| G[CUDA Graph æ•è·]
```

### 3.3.2 SamplingParams çš„ä¼ é€’è·¯å¾„

```mermaid
flowchart LR
    A[ç”¨æˆ·è°ƒç”¨ generate] -->|"SamplingParams"| B[add_request]
    B -->|"params"| C[Sequence åˆ›å»º]
    C -->|"temperature"| D[Sampler.forward]
    C -->|"max_tokens"| E[postprocess æ£€æŸ¥]
    C -->|"ignore_eos"| F[ç»ˆæ­¢æ¡ä»¶åˆ¤æ–­]
```

---

## 3.4 é…ç½®ç¤ºä¾‹

### 3.4.1 åŸºç¡€é…ç½®

```python
from nanovllm import LLM, SamplingParams

# æœ€ç®€é…ç½®
llm = LLM("/path/to/model")
params = SamplingParams()
```

### 3.4.2 é«˜æ€§èƒ½é…ç½®

```python
# å¯ç”¨ CUDA Graph ä¼˜åŒ–
llm = LLM(
    "/path/to/model",
    enforce_eager=False,  # å¯ç”¨ CUDA Graph
    tensor_parallel_size=2,  # åŒ GPU
    gpu_memory_utilization=0.95,  # æ›´é«˜æ˜¾å­˜åˆ©ç”¨ç‡
    max_num_seqs=1024,  # æ›´å¤§æ‰¹æ¬¡
)

# ä½æ¸©åº¦é‡‡æ ·ï¼ˆæ›´ç¡®å®šæ€§ï¼‰
params = SamplingParams(
    temperature=0.3,
    max_tokens=512,
)
```

### 3.4.3 è°ƒè¯•é…ç½®

```python
# ç¦ç”¨ä¼˜åŒ–ï¼Œä¾¿äºè°ƒè¯•
llm = LLM(
    "/path/to/model",
    enforce_eager=True,  # ç¦ç”¨ CUDA Graph
    tensor_parallel_size=1,  # å• GPU
)

# é«˜æ¸©åº¦é‡‡æ ·ï¼ˆæ›´å¤šæ ·æ€§ï¼‰
params = SamplingParams(
    temperature=1.5,
    max_tokens=128,
)
```

---

## 3.5 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **Config ç±»**ï¼š
   - 12 ä¸ªé…ç½®å­—æ®µçš„å«ä¹‰å’Œé»˜è®¤å€¼
   - `__post_init__` éªŒè¯é€»è¾‘
   - é…ç½®å­—æ®µä¹‹é—´çš„çº¦æŸå…³ç³»

2. **SamplingParams ç±»**ï¼š
   - 3 ä¸ªé‡‡æ ·å‚æ•°çš„ä½œç”¨
   - æ¸©åº¦é‡‡æ ·çš„æ•°å­¦åŸç†
   - ç¦æ­¢è´ªå©ªé‡‡æ ·çš„è®¾è®¡åŸå› 

3. **é…ç½®æµåŠ¨**ï¼š
   - Config ä»ç”¨æˆ·åˆ°å„ç»„ä»¶çš„ä¼ é€’è·¯å¾„
   - SamplingParams å¦‚ä½•å½±å“æ¨ç†è¿‡ç¨‹

---

**ä¸‹ä¸€ç« ** â†’ [04 åºåˆ—ä¸çŠ¶æ€ç®¡ç†](04_sequence.md)
