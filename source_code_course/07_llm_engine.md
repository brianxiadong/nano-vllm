# ç¬¬ä¸ƒç« ï¼šLLM å¼•æ“è¯¦è§£

> æœ¬ç« å°†é€è¡Œåˆ†æ `llm_engine.py`ï¼Œç†è§£æ¨ç†å¼•æ“çš„æ ¸å¿ƒå…¥å£å’Œæ‰§è¡Œå¾ªç¯ã€‚

## 7.1 LLMEngine æ¦‚è¿°

`LLMEngine` æ˜¯ Nano-vLLM çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£ï¼š

```mermaid
graph TB
    subgraph "LLMEngine èŒè´£"
        A[åˆå§‹åŒ–ç³»ç»Ÿ]
        B[ç®¡ç†å¤šè¿›ç¨‹]
        C[åè°ƒè°ƒåº¦ä¸æ‰§è¡Œ]
        D[å¤„ç†ç”Ÿæˆè¯·æ±‚]
    end
    
    subgraph "ç»„ä»¶"
        E[ModelRunner]
        F[Scheduler]
        G[Tokenizer]
    end
    
    A --> E
    A --> F
    A --> G
    B --> E
    C --> F
    C --> E
    D --> C
```

---

## 7.2 å®Œæ•´æºç 

```python
import atexit
from dataclasses import fields
from time import perf_counter
from tqdm.auto import tqdm
from transformers import AutoTokenizer
import torch.multiprocessing as mp

from nanovllm.config import Config
from nanovllm.sampling_params import SamplingParams
from nanovllm.engine.sequence import Sequence
from nanovllm.engine.scheduler import Scheduler
from nanovllm.engine.model_runner import ModelRunner


class LLMEngine:

    def __init__(self, model, **kwargs):
        config_fields = {field.name for field in fields(Config)}
        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}
        config = Config(model, **config_kwargs)
        self.ps = []
        self.events = []
        ctx = mp.get_context("spawn")
        for i in range(1, config.tensor_parallel_size):
            event = ctx.Event()
            process = ctx.Process(target=ModelRunner, args=(config, i, event))
            process.start()
            self.ps.append(process)
            self.events.append(event)
        self.model_runner = ModelRunner(config, 0, self.events)
        self.tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=True)
        config.eos = self.tokenizer.eos_token_id
        self.scheduler = Scheduler(config)
        atexit.register(self.exit)

    def exit(self):
        self.model_runner.call("exit")
        del self.model_runner
        for p in self.ps:
            p.join()

    def add_request(self, prompt: str | list[int], sampling_params: SamplingParams):
        if isinstance(prompt, str):
            prompt = self.tokenizer.encode(prompt)
        seq = Sequence(prompt, sampling_params)
        self.scheduler.add(seq)

    def step(self):
        seqs, is_prefill = self.scheduler.schedule()
        token_ids = self.model_runner.call("run", seqs, is_prefill)
        self.scheduler.postprocess(seqs, token_ids)
        outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
        num_tokens = sum(len(seq) for seq in seqs) if is_prefill else -len(seqs)
        return outputs, num_tokens

    def is_finished(self):
        return self.scheduler.is_finished()

    def generate(
        self,
        prompts: list[str] | list[list[int]],
        sampling_params: SamplingParams | list[SamplingParams],
        use_tqdm: bool = True,
    ) -> list[str]:
        if use_tqdm:
            pbar = tqdm(total=len(prompts), desc="Generating", dynamic_ncols=True)
        if not isinstance(sampling_params, list):
            sampling_params = [sampling_params] * len(prompts)
        for prompt, sp in zip(prompts, sampling_params):
            self.add_request(prompt, sp)
        outputs = {}
        prefill_throughput = decode_throughput = 0.
        while not self.is_finished():
            t = perf_counter()
            output, num_tokens = self.step()
            if use_tqdm:
                if num_tokens > 0:
                    prefill_throughput = num_tokens / (perf_counter() - t)
                else:
                    decode_throughput = -num_tokens / (perf_counter() - t)
                pbar.set_postfix({
                    "Prefill": f"{int(prefill_throughput)}tok/s",
                    "Decode": f"{int(decode_throughput)}tok/s",
                })
            for seq_id, token_ids in output:
                outputs[seq_id] = token_ids
                if use_tqdm:
                    pbar.update(1)
        outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]
        outputs = [{"text": self.tokenizer.decode(token_ids), "token_ids": token_ids} for token_ids in outputs]
        if use_tqdm:
            pbar.close()
        return outputs
```

---

## 7.3 æ„é€ å‡½æ•°è¯¦è§£

### 7.3.1 é…ç½®è§£æ

```python
def __init__(self, model, **kwargs):
    config_fields = {field.name for field in fields(Config)}
    config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}
    config = Config(model, **config_kwargs)
```

**é€è¡Œè§£æ**ï¼š

| è¡Œå· | ä»£ç  | è¯´æ˜ |
|:---:|:---|:---|
| 1 | `fields(Config)` | è·å– Config ç±»çš„æ‰€æœ‰å­—æ®µ |
| 2 | å­—å…¸æ¨å¯¼å¼ | åªä¿ç•™ Config æ”¯æŒçš„å‚æ•° |
| 3 | åˆ›å»º Config | ä½¿ç”¨æœ‰æ•ˆå‚æ•°åˆå§‹åŒ– |

**ä½œç”¨**ï¼šè¿‡æ»¤æ— æ•ˆå‚æ•°ï¼Œé¿å…ä¼ å…¥æœªçŸ¥å‚æ•°å¯¼è‡´æŠ¥é”™ã€‚

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šå‚æ•°è¿‡æ»¤è®© API æ›´å¥å£®â€”â€”ç”¨æˆ·å¯ä»¥ä¼ é€’é¢å¤–å‚æ•°è€Œä¸ä¼šå‡ºé”™ï¼Œè¿™å¯¹äºå°†æ¥æ·»åŠ æ–°é…ç½®å­—æ®µæ—¶çš„å‘åå…¼å®¹æ€§å¾ˆé‡è¦ã€‚

### 7.3.2 å¤šè¿›ç¨‹åˆå§‹åŒ–

```python
    self.ps = []                           # å­è¿›ç¨‹åˆ—è¡¨
    self.events = []                       # åŒæ­¥äº‹ä»¶åˆ—è¡¨
    ctx = mp.get_context("spawn")          # ä½¿ç”¨ spawn æ¨¡å¼
    
    for i in range(1, config.tensor_parallel_size):
        event = ctx.Event()                # åˆ›å»ºåŒæ­¥äº‹ä»¶
        process = ctx.Process(target=ModelRunner, args=(config, i, event))
        process.start()                    # å¯åŠ¨å­è¿›ç¨‹
        self.ps.append(process)
        self.events.append(event)
    
    self.model_runner = ModelRunner(config, 0, self.events)  # ä¸»è¿›ç¨‹
```

**å¤šè¿›ç¨‹æ¶æ„**ï¼š

```mermaid
graph TB
    subgraph "ä¸»è¿›ç¨‹ Rank 0"
        A[ModelRunner]
        B[LLMEngine]
        C[events åˆ—è¡¨]
    end
    
    subgraph "å­è¿›ç¨‹ Rank 1"
        D[ModelRunner]
        E[event]
    end
    
    subgraph "å­è¿›ç¨‹ Rank 2"
        F[ModelRunner]
        G[event]
    end
    
    B --> A
    C --> E
    C --> G
    A -.->|"NCCL"| D
    A -.->|"NCCL"| F
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ `spawn`ï¼Ÿ**

| æ¨¡å¼ | è¯´æ˜ | åŸå›  |
|:---|:---|:---|
| `spawn` | å¯åŠ¨æ–° Python è§£é‡Šå™¨ | CUDA è¦æ±‚ï¼Œé¿å…å†…å­˜å…±äº«é—®é¢˜ |
| `fork` | å¤åˆ¶çˆ¶è¿›ç¨‹ | åœ¨ CUDA ç¯å¢ƒä¸‹ä¸å®‰å…¨ |

### 7.3.3 ç»„ä»¶åˆå§‹åŒ–

```python
    self.tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=True)
    config.eos = self.tokenizer.eos_token_id   # è®¾ç½® EOS token
    self.scheduler = Scheduler(config)         # åˆ›å»ºè°ƒåº¦å™¨
    atexit.register(self.exit)                 # æ³¨å†Œé€€å‡ºå¤„ç†
```

**`atexit.register`**ï¼šç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨è°ƒç”¨ `exit()` æ–¹æ³•ï¼Œç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾ã€‚

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šä½¿ç”¨ `atexit.register` è€Œéä¾èµ–ç”¨æˆ·æ˜¾å¼è°ƒç”¨ `close()`ï¼Œä½“ç°äº†ã€Œèµ„æºè‡ªåŠ¨ç®¡ç†ã€æ€æƒ³ã€‚å³ä½¿ç¨‹åºå¼‚å¸¸é€€å‡ºï¼ŒGPU è¿›ç¨‹ä¹Ÿèƒ½è¢«æ­£ç¡®æ¸…ç†ã€‚

---

## 7.4 é€€å‡ºå¤„ç†

```python
def exit(self):
    self.model_runner.call("exit")    # é€šçŸ¥æ‰€æœ‰è¿›ç¨‹é€€å‡º
    del self.model_runner             # åˆ é™¤ä¸»è¿›ç¨‹ runner
    for p in self.ps:
        p.join()                      # ç­‰å¾…å­è¿›ç¨‹ç»“æŸ
```

**é€€å‡ºæµç¨‹**ï¼š

```mermaid
sequenceDiagram
    participant Main as ä¸»è¿›ç¨‹
    participant R0 as Rank 0 Runner
    participant R1 as Rank 1
    participant R2 as Rank 2
    
    Main->>R0: call("exit")
    R0->>R0: write_shm("exit")
    R0->>R1: event.set()
    R0->>R2: event.set()
    
    par å¹¶è¡Œé€€å‡º
        R0->>R0: exit()
        R1->>R1: exit()
        R2->>R2: exit()
    end
    
    Main->>Main: del model_runner
    Main->>R1: p.join()
    Main->>R2: p.join()
```

---

## 7.5 æ·»åŠ è¯·æ±‚

```python
def add_request(self, prompt: str | list[int], sampling_params: SamplingParams):
    if isinstance(prompt, str):
        prompt = self.tokenizer.encode(prompt)   # å­—ç¬¦ä¸²è½¬ token IDs
    seq = Sequence(prompt, sampling_params)      # åˆ›å»ºåºåˆ—å¯¹è±¡
    self.scheduler.add(seq)                      # æ·»åŠ åˆ°è°ƒåº¦å™¨
```

**ç±»å‹æ”¯æŒ**ï¼š

```mermaid
graph TD
    A[prompt è¾“å…¥] --> B{ç±»å‹?}
    B -->|str| C[tokenizer.encode]
    B -->|list#91;int#93;| D[ç›´æ¥ä½¿ç”¨]
    C --> E[åˆ›å»º Sequence]
    D --> E
    E --> F[scheduler.add]
```

---

## 7.6 å•æ­¥æ‰§è¡Œ

```python
def step(self):
    seqs, is_prefill = self.scheduler.schedule()     # 1. è°ƒåº¦
    token_ids = self.model_runner.call("run", seqs, is_prefill)  # 2. æ‰§è¡Œ
    self.scheduler.postprocess(seqs, token_ids)      # 3. åå¤„ç†
    
    # æ”¶é›†å®Œæˆçš„åºåˆ—
    outputs = [(seq.seq_id, seq.completion_token_ids) for seq in seqs if seq.is_finished]
    
    # è®¡ç®— token æ•°ï¼ˆç”¨äºååé‡ç»Ÿè®¡ï¼‰
    num_tokens = sum(len(seq) for seq in seqs) if is_prefill else -len(seqs)
    
    return outputs, num_tokens
```

**é€è¡Œè§£æ**ï¼š

| æ­¥éª¤ | ä»£ç  | è¾“å‡º |
|:---:|:---|:---|
| è°ƒåº¦ | `scheduler.schedule()` | æœ¬è½®åºåˆ—ã€æ˜¯å¦ Prefill |
| æ‰§è¡Œ | `model_runner.call("run", ...)` | ç”Ÿæˆçš„ token IDs |
| åå¤„ç† | `scheduler.postprocess(...)` | æ›´æ–°åºåˆ—çŠ¶æ€ |
| æ”¶é›† | åˆ—è¡¨æ¨å¯¼ | å·²å®Œæˆçš„ (seq_id, tokens) |

**`num_tokens` çš„å«ä¹‰**ï¼š

| å€¼ | é˜¶æ®µ | è¯´æ˜ |
|:---|:---|:---|
| > 0 | Prefill | å¤„ç†çš„æ€» token æ•° |
| < 0 | Decode | è´Ÿçš„åºåˆ—æ•°ï¼ˆæ¯åºåˆ— 1 tokenï¼‰ |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼š`step()` æ–¹æ³•å°†è°ƒåº¦ã€æ‰§è¡Œã€åå¤„ç†ä¸‰æ­¥ç»Ÿä¸€ï¼Œå½¢æˆæ¸…æ™°çš„ã€Œå•æ­¥æ‰§è¡Œã€æŠ½è±¡ã€‚è¿”å› `num_tokens` ä½¿ç”¨æ­£è´Ÿå·åŒºåˆ† Prefill/Decode æ˜¯å·§å¦™çš„å¤ç”¨è®¾è®¡ã€‚

---

## 7.7 ç”Ÿæˆä¸»å¾ªç¯

```python
def generate(
    self,
    prompts: list[str] | list[list[int]],
    sampling_params: SamplingParams | list[SamplingParams],
    use_tqdm: bool = True,
) -> list[str]:
```

### 7.7.1 åˆå§‹åŒ–é˜¶æ®µ

```python
    if use_tqdm:
        pbar = tqdm(total=len(prompts), desc="Generating", dynamic_ncols=True)
    
    if not isinstance(sampling_params, list):
        sampling_params = [sampling_params] * len(prompts)
    
    for prompt, sp in zip(prompts, sampling_params):
        self.add_request(prompt, sp)
    
    outputs = {}
    prefill_throughput = decode_throughput = 0.
```

**æµç¨‹**ï¼š

1. åˆ›å»ºè¿›åº¦æ¡
2. ç»Ÿä¸€ sampling_params æ ¼å¼ï¼ˆå•ä¸ª â†’ åˆ—è¡¨ï¼‰
3. æ·»åŠ æ‰€æœ‰è¯·æ±‚åˆ°è°ƒåº¦å™¨
4. åˆå§‹åŒ–è¾“å‡ºå­—å…¸å’Œååé‡è®¡æ•°å™¨

### 7.7.2 æ‰§è¡Œå¾ªç¯

```python
    while not self.is_finished():
        t = perf_counter()
        output, num_tokens = self.step()
        
        if use_tqdm:
            if num_tokens > 0:
                prefill_throughput = num_tokens / (perf_counter() - t)
            else:
                decode_throughput = -num_tokens / (perf_counter() - t)
            pbar.set_postfix({
                "Prefill": f"{int(prefill_throughput)}tok/s",
                "Decode": f"{int(decode_throughput)}tok/s",
            })
        
        for seq_id, token_ids in output:
            outputs[seq_id] = token_ids
            if use_tqdm:
                pbar.update(1)
```

**æ‰§è¡Œå¾ªç¯æµç¨‹**ï¼š

```mermaid
flowchart TD
    A[å¼€å§‹] --> B{is_finished?}
    B -->|æ˜¯| G[ç»“æŸå¾ªç¯]
    B -->|å¦| C[è®°å½•æ—¶é—´]
    C --> D[æ‰§è¡Œ step]
    D --> E[è®¡ç®—ååé‡]
    E --> F[æ”¶é›†å®Œæˆè¾“å‡º]
    F --> B
```

### 7.7.3 ç»“æœå¤„ç†

```python
    outputs = [outputs[seq_id] for seq_id in sorted(outputs.keys())]
    outputs = [{"text": self.tokenizer.decode(token_ids), "token_ids": token_ids} 
               for token_ids in outputs]
    if use_tqdm:
        pbar.close()
    return outputs
```

**å¤„ç†æ­¥éª¤**ï¼š

1. **æ’åº**ï¼šæŒ‰ seq_id æ’åºï¼Œä¿è¯è¾“å‡ºé¡ºåºä¸è¾“å…¥ä¸€è‡´
2. **è§£ç **ï¼štoken IDs â†’ æ–‡æœ¬
3. **æ ¼å¼åŒ–**ï¼šè¿”å›åŒ…å« text å’Œ token_ids çš„å­—å…¸

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šä½¿ç”¨ `seq_id` ä½œä¸ºæ’åºä¾æ®è€Œéä¿æŒæ’å…¥é¡ºåºï¼Œæ˜¯å› ä¸º Continuous Batching ä¸‹åºåˆ—å®Œæˆé¡ºåºæ— æ³•é¢„çŸ¥ã€‚ç”¨å­—å…¸æ”¶é›†ç»“æœå†æ’åºï¼Œç®€å•ä¸”æ­£ç¡®ã€‚

---

## 7.8 å®Œæ•´æ‰§è¡Œæµç¨‹

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Gen as generate()
    participant Add as add_request()
    participant Step as step()
    participant Sched as Scheduler
    participant Runner as ModelRunner
    
    User->>Gen: generate(prompts, params)
    
    loop æ¯ä¸ª prompt
        Gen->>Add: add_request(prompt, sp)
        Add->>Sched: scheduler.add(seq)
    end
    
    loop ç›´åˆ° is_finished()
        Gen->>Step: step()
        Step->>Sched: schedule()
        Sched-->>Step: (seqs, is_prefill)
        Step->>Runner: call("run", seqs, is_prefill)
        Runner-->>Step: token_ids
        Step->>Sched: postprocess(seqs, token_ids)
        Step-->>Gen: (outputs, num_tokens)
        Gen->>Gen: æ›´æ–°è¿›åº¦æ¡
    end
    
    Gen->>Gen: æ’åºã€è§£ç 
    Gen-->>User: outputs
```

---

## 7.9 ååé‡è®¡ç®—

### 7.9.1 è®¡ç®—é€»è¾‘

```python
if num_tokens > 0:
    prefill_throughput = num_tokens / (perf_counter() - t)
else:
    decode_throughput = -num_tokens / (perf_counter() - t)
```

### 7.9.2 ç¤ºä¾‹

**Prefill é˜¶æ®µ**ï¼š

```
seqs = [Seq1(100), Seq2(200), Seq3(150)]
num_tokens = 100 + 200 + 150 = 450
æ—¶é—´ = 0.5s
prefill_throughput = 450 / 0.5 = 900 tok/s
```

**Decode é˜¶æ®µ**ï¼š

```
seqs = [Seq1, Seq2, Seq3]  # æ¯ä¸ªç”Ÿæˆ 1 ä¸ª token
num_tokens = -3
æ—¶é—´ = 0.01s
decode_throughput = 3 / 0.01 = 300 tok/s
```

---

## 7.10 è¾“å‡ºæ ¼å¼

```python
outputs = [
    {
        "text": "ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹...",
        "token_ids": [12, 345, 67, 89, 1234, ...]
    },
    {
        "text": "å¦ä¸€ä¸ªè¾“å‡º...",
        "token_ids": [98, 76, 54, ...]
    }
]
```

**ä¸ vLLM çš„å¯¹æ¯”**ï¼š

| æ¡†æ¶ | è¿”å›æ ¼å¼ |
|:---|:---|
| vLLM | `RequestOutput` å¯¹è±¡ |
| Nano-vLLM | å­—å…¸åˆ—è¡¨ |

---

## 7.11 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **æ„é€ å‡½æ•°**ï¼š
   - é…ç½®è§£æä¸è¿‡æ»¤
   - å¤šè¿›ç¨‹åˆå§‹åŒ–ï¼ˆspawn æ¨¡å¼ï¼‰
   - ç»„ä»¶åˆå§‹åŒ–é¡ºåº

2. **é€€å‡ºå¤„ç†**ï¼š
   - `atexit.register` ç¡®ä¿èµ„æºé‡Šæ”¾
   - å¤šè¿›ç¨‹åŒæ­¥é€€å‡º

3. **æ ¸å¿ƒæ–¹æ³•**ï¼š
   - `add_request`ï¼šæ·»åŠ æ¨ç†è¯·æ±‚
   - `step`ï¼šå•æ­¥æ‰§è¡Œï¼ˆè°ƒåº¦â†’æ‰§è¡Œâ†’åå¤„ç†ï¼‰
   - `generate`ï¼šå®Œæ•´ç”Ÿæˆå¾ªç¯

4. **ååé‡ç»Ÿè®¡**ï¼š
   - Prefillï¼šæ€» token æ•° / æ—¶é—´
   - Decodeï¼šåºåˆ—æ•° / æ—¶é—´

5. **è¾“å‡ºæ ¼å¼**ï¼š
   - æŒ‰è¾“å…¥é¡ºåºæ’åº
   - åŒ…å« text å’Œ token_ids

---

**ä¸‹ä¸€ç« ** â†’ [08 æ¨¡å‹è¿è¡Œå™¨](08_model_runner.md)
