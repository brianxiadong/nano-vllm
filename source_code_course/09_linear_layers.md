# ç¬¬ä¹ç« ï¼šçº¿æ€§å±‚ä¸å¼ é‡å¹¶è¡Œ

> æœ¬ç« å°†é€è¡Œåˆ†æ `linear.py`ï¼Œç†è§£å¹¶è¡Œçº¿æ€§å±‚çš„å®ç°å’Œå¼ é‡å¹¶è¡Œç­–ç•¥ã€‚

## 9.1 å¼ é‡å¹¶è¡Œæ¦‚è¿°

å¼ é‡å¹¶è¡Œå°†æ¨¡å‹çš„æƒé‡çŸ©é˜µåˆ‡åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼š

```mermaid
graph LR
    subgraph "åˆ—å¹¶è¡Œ"
        A[è¾“å…¥ X] --> B["Wâ‚ (GPU 0)"]
        A --> C["Wâ‚‚ (GPU 1)"]
        B --> D["Yâ‚"]
        C --> E["Yâ‚‚"]
        D --> F["concat"]
        E --> F
    end
    
    subgraph "è¡Œå¹¶è¡Œ"
        G[è¾“å…¥ X] --> H["Wâ‚ (GPU 0)"]
        G --> I["Wâ‚‚ (GPU 1)"]
        H --> J["Yâ‚"]
        I --> K["Yâ‚‚"]
        J --> L["AllReduce"]
        K --> L
    end
```

---

## 9.2 è¾…åŠ©å‡½æ•°

```python
def divide(numerator, denominator):
    assert numerator % denominator == 0
    return numerator // denominator
```

ç¡®ä¿ç»´åº¦å¯ä»¥è¢«å‡åŒ€åˆ‡åˆ†ã€‚

---

## 9.3 LinearBase åŸºç±»

```python
class LinearBase(nn.Module):

    def __init__(
        self,
        input_size: int,
        output_size: int,
        bias: bool = False,
        tp_dim: int | None = None,
    ):
        super().__init__()
        self.tp_dim = tp_dim                              # å¹¶è¡Œç»´åº¦
        self.tp_rank = dist.get_rank()                    # å½“å‰è¿›ç¨‹ ID
        self.tp_size = dist.get_world_size()              # æ€»è¿›ç¨‹æ•°
        self.weight = nn.Parameter(torch.empty(output_size, input_size))
        self.weight.weight_loader = self.weight_loader    # æƒé‡åŠ è½½å™¨
        if bias:
            self.bias = nn.Parameter(torch.empty(output_size))
            self.bias.weight_loader = self.weight_loader
        else:
            self.register_parameter("bias", None)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        raise NotImplementedError
```

### å…³é”®å±æ€§

| å±æ€§ | è¯´æ˜ |
|:---|:---|
| `tp_dim` | å¹¶è¡Œåˆ‡åˆ†çš„ç»´åº¦ï¼ˆ0=åˆ—ï¼Œ1=è¡Œï¼‰ |
| `tp_rank` | å½“å‰ GPU ID |
| `tp_size` | æ€» GPU æ•° |
| `weight_loader` | è‡ªå®šä¹‰æƒé‡åŠ è½½æ–¹æ³• |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼š`LinearBase` åŸºç±»å°è£…äº†å¹¶è¡Œé…ç½®çš„é€šç”¨é€»è¾‘ï¼Œå­ç±»åªéœ€å®ç°å…·ä½“çš„åˆ‡åˆ†ç­–ç•¥ã€‚`weight_loader` ä½œä¸ºå±æ€§é™„åŠ åˆ°æ¯ä¸ªå‚æ•°ä¸Šï¼Œæ˜¯å…¸å‹çš„ã€Œç­–ç•¥æ¨¡å¼ã€åº”ç”¨ã€‚

---

## 9.4 ReplicatedLinear

```python
class ReplicatedLinear(LinearBase):

    def __init__(
        self,
        input_size: int,
        output_size: int,
        bias: bool = False,
    ):
        super().__init__(input_size, output_size, bias)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
        param.data.copy_(loaded_weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.linear(x, self.weight, self.bias)
```

**ç‰¹ç‚¹**ï¼šæ¯ä¸ª GPU æŒæœ‰å®Œæ•´æƒé‡ï¼Œæ— éœ€é€šä¿¡ã€‚

```mermaid
graph LR
    subgraph "GPU 0"
        A0[å®Œæ•´ W]
    end
    subgraph "GPU 1"
        A1[å®Œæ•´ W]
    end
    X[è¾“å…¥] --> A0 --> Y0[è¾“å‡º]
    X --> A1 --> Y1[è¾“å‡º]
```

---

## 9.5 ColumnParallelLinear

```python
class ColumnParallelLinear(LinearBase):

    def __init__(
        self,
        input_size: int,
        output_size: int,
        bias: bool = False,
    ):
        tp_size = dist.get_world_size()
        super().__init__(input_size, divide(output_size, tp_size), bias, 0)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
        param_data = param.data
        shard_size = param_data.size(self.tp_dim)
        start_idx = self.tp_rank * shard_size
        loaded_weight = loaded_weight.narrow(self.tp_dim, start_idx, shard_size)
        param_data.copy_(loaded_weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.linear(x, self.weight, self.bias)
```

### é€è¡Œåˆ†æ

**æ„é€ å‡½æ•°**ï¼š

```python
tp_size = dist.get_world_size()
# output_size è¢«åˆ‡åˆ†ä¸º output_size // tp_size
super().__init__(input_size, divide(output_size, tp_size), bias, 0)
```

**æƒé‡åŠ è½½**ï¼š

```python
shard_size = param_data.size(self.tp_dim)  # åˆ‡ç‰‡å¤§å°
start_idx = self.tp_rank * shard_size       # èµ·å§‹ä½ç½®
loaded_weight = loaded_weight.narrow(self.tp_dim, start_idx, shard_size)
```

### åˆ—å¹¶è¡Œç¤ºæ„å›¾

```mermaid
graph TB
    subgraph "åŸå§‹æƒé‡ W [out, in]"
        A["[4096, 768]"]
    end
    
    subgraph "GPU 0"
        B["Wâ‚€ [2048, 768]"]
    end
    
    subgraph "GPU 1"
        C["Wâ‚ [2048, 768]"]
    end
    
    A -->|"narrow(0, 0, 2048)"| B
    A -->|"narrow(0, 2048, 2048)"| C
```

---

## 9.6 MergedColumnParallelLinear

```python
class MergedColumnParallelLinear(ColumnParallelLinear):

    def __init__(
        self,
        input_size: int,
        output_sizes: list[int],
        bias: bool = False,
    ):
        self.output_sizes = output_sizes
        super().__init__(input_size, sum(output_sizes), bias)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, loaded_shard_id: int):
        param_data = param.data
        shard_offset = sum(self.output_sizes[:loaded_shard_id]) // self.tp_size
        shard_size = self.output_sizes[loaded_shard_id] // self.tp_size
        param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)
        loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]
        param_data.copy_(loaded_weight)
```

### ç”¨é€”

ç”¨äºåˆå¹¶å¤šä¸ªæŠ•å½±å±‚ï¼Œå¦‚ MLP ä¸­çš„ `gate_proj` å’Œ `up_proj`ï¼š

```mermaid
graph TB
    subgraph "åŸå§‹"
        A["gate_proj [11008, 4096]"]
        B["up_proj [11008, 4096]"]
    end
    
    subgraph "åˆå¹¶"
        C["gate_up_proj [22016, 4096]"]
    end
    
    A --> C
    B --> C
```

### æƒé‡åŠ è½½å‚æ•°

```python
weight_loader(param, loaded_weight, loaded_shard_id)
```

| å‚æ•° | è¯´æ˜ |
|:---|:---|
| `loaded_shard_id` | 0 = gate_proj, 1 = up_proj |
| `shard_offset` | åœ¨åˆå¹¶æƒé‡ä¸­çš„åç§» |
| `shard_size` | åˆ‡ç‰‡å¤§å° |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šåˆå¹¶å¤šä¸ªæŠ•å½±å±‚å‡å°‘äº†å†…å­˜è®¿é—®æ¬¡æ•°â€”â€”ä¸€æ¬¡ `F.linear` è°ƒç”¨åŒæ—¶è®¡ç®— gate å’Œ upï¼Œè€Œéä¸¤æ¬¡ç®—ç‹¬ç«‹è®¡ç®—ã€‚è¿™å¯ä»¥æ˜¾è‘—æå‡ GPU åˆ©ç”¨ç‡ã€‚

---

## 9.7 QKVParallelLinear

```python
class QKVParallelLinear(ColumnParallelLinear):

    def __init__(
        self,
        hidden_size: int,
        head_size: int,
        total_num_heads: int,
        total_num_kv_heads: int | None = None,
        bias: bool = False,
    ):
        tp_size = dist.get_world_size()
        total_num_kv_heads = total_num_kv_heads or total_num_heads
        self.head_size = head_size
        self.num_heads = divide(total_num_heads, tp_size)
        self.num_kv_heads = divide(total_num_kv_heads, tp_size)
        output_size = (total_num_heads + 2 * total_num_kv_heads) * self.head_size
        super().__init__(hidden_size, output_size, bias)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, loaded_shard_id: str):
        param_data = param.data
        assert loaded_shard_id in ["q", "k", "v"]
        
        if loaded_shard_id == "q":
            shard_size = self.num_heads * self.head_size
            shard_offset = 0
        elif loaded_shard_id == "k":
            shard_size = self.num_kv_heads * self.head_size
            shard_offset = self.num_heads * self.head_size
        else:  # "v"
            shard_size = self.num_kv_heads * self.head_size
            shard_offset = self.num_heads * self.head_size + self.num_kv_heads * self.head_size
        
        param_data = param_data.narrow(self.tp_dim, shard_offset, shard_size)
        loaded_weight = loaded_weight.chunk(self.tp_size, self.tp_dim)[self.tp_rank]
        param_data.copy_(loaded_weight)
```

### QKV å†…å­˜å¸ƒå±€

```mermaid
graph TB
    subgraph "QKV åˆå¹¶æƒé‡"
        A["Q heads"] --> B["K heads"] --> C["V heads"]
    end
    
    subgraph "GPU 0 (tp_rank=0)"
        D["Q heads 0-3"]
        E["K heads 0-1"]
        F["V heads 0-1"]
    end
    
    subgraph "GPU 1 (tp_rank=1)"
        G["Q heads 4-7"]
        H["K heads 2-3"]
        I["V heads 2-3"]
    end
```

### Grouped Query Attention (GQA)

| é…ç½® | Q å¤´æ•° | KV å¤´æ•° | è¯´æ˜ |
|:---|:---|:---|:---|
| MHA | 8 | 8 | Multi-Head Attention |
| GQA | 8 | 4 | Grouped Query Attention |
| MQA | 8 | 1 | Multi-Query Attention |

---

## 9.8 RowParallelLinear

```python
class RowParallelLinear(LinearBase):

    def __init__(
        self,
        input_size: int,
        output_size: int,
        bias: bool = False,
    ):
        tp_size = dist.get_world_size()
        super().__init__(divide(input_size, tp_size), output_size, bias, 1)

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
        param_data = param.data
        shard_size = param_data.size(self.tp_dim)
        start_idx = self.tp_rank * shard_size
        loaded_weight = loaded_weight.narrow(self.tp_dim, start_idx, shard_size)
        param_data.copy_(loaded_weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        y = F.linear(x, self.weight, self.bias if self.tp_rank == 0 else None)
        if self.tp_size > 1:
            dist.all_reduce(y)
        return y
```

### é€è¡Œåˆ†æ

**æ„é€ å‡½æ•°**ï¼š

```python
# input_size è¢«åˆ‡åˆ†
super().__init__(divide(input_size, tp_size), output_size, bias, 1)
```

**å‰å‘ä¼ æ’­**ï¼š

```python
# bias åªåœ¨ rank 0 æ·»åŠ ï¼ˆé¿å…é‡å¤ï¼‰
y = F.linear(x, self.weight, self.bias if self.tp_rank == 0 else None)

# AllReduce æ±‚å’Œ
if self.tp_size > 1:
    dist.all_reduce(y)
```

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šè¡Œå¹¶è¡Œçš„å…³é”®æ˜¯ `AllReduce` æ±‚å’Œâ€”â€”æ•°å­¦ä¸Šï¼ŒçŸ©é˜µä¹˜æ³• $(X_0 Â· W_0) + (X_1 Â· W_1) = X Â· W$ã€‚åªåœ¨ rank 0 æ·»åŠ  bias é¿å…é‡å¤è®¡ç®—ï¼Œå› ä¸º AllReduce ä¼šæ±‚å’Œã€‚

### è¡Œå¹¶è¡Œç¤ºæ„å›¾

```mermaid
graph TB
    subgraph "è¾“å…¥åˆ‡åˆ†"
        X["X [batch, 4096]"]
        X0["Xâ‚€ [batch, 2048]"]
        X1["Xâ‚ [batch, 2048]"]
    end
    
    subgraph "GPU 0"
        W0["Wâ‚€ [2048, hidden]"]
        Y0["Yâ‚€ = Xâ‚€ @ Wâ‚€"]
    end
    
    subgraph "GPU 1"
        W1["Wâ‚ [2048, hidden]"]
        Y1["Yâ‚ = Xâ‚ @ Wâ‚"]
    end
    
    subgraph "AllReduce"
        AR["Y = Yâ‚€ + Yâ‚"]
    end
    
    X --> X0
    X --> X1
    X0 --> W0 --> Y0
    X1 --> W1 --> Y1
    Y0 --> AR
    Y1 --> AR
```

---

## 9.9 æƒé‡åŠ è½½è®¾è®¡

### è®¾è®¡æ¨¡å¼

```python
self.weight.weight_loader = self.weight_loader
```

æ¯ä¸ªå‚æ•°éƒ½é™„åŠ äº†ä¸€ä¸ª `weight_loader` æ–¹æ³•ï¼Œåœ¨ `loader.py` ä¸­è°ƒç”¨ï¼š

```python
weight_loader = getattr(param, "weight_loader", default_weight_loader)
weight_loader(param, loaded_weight, shard_id)  # å¦‚æœéœ€è¦ shard_id
```

### ä¼˜åŠ¿

1. **è§£è€¦**ï¼šåŠ è½½é€»è¾‘ä¸æ¨¡å‹å®šä¹‰åˆ†ç¦»
2. **çµæ´»**ï¼šæ¯ç§å¹¶è¡Œç­–ç•¥æœ‰è‡ªå·±çš„åŠ è½½æ–¹æ³•
3. **å…¼å®¹**ï¼šå…¼å®¹ HuggingFace æ¨¡å‹æƒé‡æ ¼å¼

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šå°†åŠ è½½é€»è¾‘å°è£…åœ¨å‚æ•°è‡ªèº«è€Œéæ¨¡å‹çº§åˆ«ï¼Œè®©ä¸åŒå±‚å¯ä»¥æœ‰ä¸åŒçš„åŠ è½½ç­–ç•¥ã€‚è¿™ç§ã€Œè¡Œä¸ºé™„åŠ åœ¨æ•°æ®ä¸Šã€çš„æ¨¡å¼å¾ˆç¬¦åˆ Python çš„åŠ¨æ€ç‰¹æ€§ã€‚

---

## 9.10 å¹¶è¡Œç­–ç•¥æ€»ç»“

| å±‚ç±»å‹ | åˆ‡åˆ†ç»´åº¦ | é€šä¿¡ | ç”¨é€” |
|:---|:---|:---|:---|
| Replicated | æ—  | æ—  | å°å‹æƒé‡ |
| ColumnParallel | output | æ—  | ç¬¬ä¸€ä¸ªçº¿æ€§å±‚ |
| MergedColumn | output | æ—  | gate_up_proj |
| QKVParallel | output | æ—  | QKV æŠ•å½± |
| RowParallel | input | AllReduce | ç¬¬äºŒä¸ªçº¿æ€§å±‚ |

### Transformer å—ä¸­çš„å¹¶è¡Œç­–ç•¥

```mermaid
graph LR
    subgraph "Attention"
        A[QKV Proj] -->|ColumnParallel| B[Attention]
        B --> C[O Proj]
        C -->|RowParallel| D[è¾“å‡º]
    end
    
    subgraph "MLP"
        E[Gate+Up] -->|MergedColumn| F[æ¿€æ´»]
        F --> G[Down]
        G -->|RowParallel| H[è¾“å‡º]
    end
```

---

## 9.11 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **å¼ é‡å¹¶è¡ŒåŸºç¡€**ï¼šåˆ—å¹¶è¡Œå’Œè¡Œå¹¶è¡Œçš„åŒºåˆ«
2. **LinearBase**ï¼šå¹¶è¡Œçº¿æ€§å±‚çš„åŸºç±»è®¾è®¡
3. **ColumnParallelLinear**ï¼šæŒ‰è¾“å‡ºç»´åº¦åˆ‡åˆ†
4. **MergedColumnParallelLinear**ï¼šåˆå¹¶å¤šä¸ªæŠ•å½±
5. **QKVParallelLinear**ï¼šå¤„ç† GQA/MQA
6. **RowParallelLinear**ï¼šæŒ‰è¾“å…¥ç»´åº¦åˆ‡åˆ† + AllReduce
7. **æƒé‡åŠ è½½**ï¼š`weight_loader` è®¾è®¡æ¨¡å¼

---

**ä¸‹ä¸€ç« ** â†’ [10 æ³¨æ„åŠ›æœºåˆ¶](10_attention.md)
