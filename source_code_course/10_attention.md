# ç¬¬åç« ï¼šæ³¨æ„åŠ›æœºåˆ¶

> æœ¬ç« å°†é€è¡Œåˆ†æ `attention.py`ï¼Œç†è§£ Flash Attention é›†æˆå’Œ KV Cache å­˜å‚¨ã€‚

## 10.1 æ³¨æ„åŠ›æœºåˆ¶æ¦‚è¿°

Nano-vLLM çš„æ³¨æ„åŠ›å®ç°æœ‰ä¸¤ä¸ªå…³é”®ç‰¹ç‚¹ï¼š

```mermaid
graph TB
    subgraph "Attention æ¨¡å—"
        A[Flash Attention]
        B[KV Cache å­˜å‚¨]
    end
    
    subgraph "Prefill"
        C["flash_attn_varlen_func"]
    end
    
    subgraph "Decode"
        D["flash_attn_with_kvcache"]
    end
    
    A --> C
    A --> D
    B --> C
    B --> D
```

---

## 10.2 å®Œæ•´æºç 

```python
import torch
from torch import nn
import triton
import triton.language as tl

from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache
from nanovllm.utils.context import get_context


@triton.jit
def store_kvcache_kernel(
    key_ptr,
    key_stride,
    value_ptr,
    value_stride,
    k_cache_ptr,
    v_cache_ptr,
    slot_mapping_ptr,
    D: tl.constexpr,
):
    idx = tl.program_id(0)
    slot = tl.load(slot_mapping_ptr + idx)
    if slot == -1: return
    key_offsets = idx * key_stride + tl.arange(0, D)
    value_offsets = idx * value_stride + tl.arange(0, D)
    key = tl.load(key_ptr + key_offsets)
    value = tl.load(value_ptr + value_offsets)
    cache_offsets = slot * D + tl.arange(0, D)
    tl.store(k_cache_ptr + cache_offsets, key)
    tl.store(v_cache_ptr + cache_offsets, value)


def store_kvcache(key: torch.Tensor, value: torch.Tensor, k_cache: torch.Tensor, 
                  v_cache: torch.Tensor, slot_mapping: torch.Tensor):
    N, num_heads, head_dim = key.shape
    D = num_heads * head_dim
    assert key.stride(-1) == 1 and value.stride(-1) == 1
    assert key.stride(1) == head_dim and value.stride(1) == head_dim
    assert k_cache.stride(1) == D and v_cache.stride(1) == D
    assert slot_mapping.numel() == N
    store_kvcache_kernel[(N,)](key, key.stride(0), value, value.stride(0), 
                               k_cache, v_cache, slot_mapping, D)


class Attention(nn.Module):

    def __init__(
        self,
        num_heads,
        head_dim,
        scale,
        num_kv_heads,
    ):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.scale = scale
        self.num_kv_heads = num_kv_heads
        self.k_cache = self.v_cache = torch.tensor([])

    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):
        context = get_context()
        k_cache, v_cache = self.k_cache, self.v_cache
        if k_cache.numel() and v_cache.numel():
            store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)
        if context.is_prefill:
            if context.block_tables is not None:    # prefix cache
                k, v = k_cache, v_cache
            o = flash_attn_varlen_func(q, k, v,
                                       max_seqlen_q=context.max_seqlen_q, 
                                       cu_seqlens_q=context.cu_seqlens_q,
                                       max_seqlen_k=context.max_seqlen_k, 
                                       cu_seqlens_k=context.cu_seqlens_k,
                                       softmax_scale=self.scale, causal=True, 
                                       block_table=context.block_tables)
        else:    # decode
            o = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache,
                                        cache_seqlens=context.context_lens, 
                                        block_table=context.block_tables, 
                                        softmax_scale=self.scale, causal=True)
        return o
```

---

## 10.3 Triton Kernelï¼šKV Cache å­˜å‚¨

```python
@triton.jit
def store_kvcache_kernel(
    key_ptr,
    key_stride,
    value_ptr,
    value_stride,
    k_cache_ptr,
    v_cache_ptr,
    slot_mapping_ptr,
    D: tl.constexpr,
):
    idx = tl.program_id(0)                           # å½“å‰çº¿ç¨‹å¤„ç†çš„ token ç´¢å¼•
    slot = tl.load(slot_mapping_ptr + idx)           # è¯»å–ç›®æ ‡ slot
    if slot == -1: return                            # æ— æ•ˆ slotï¼Œè·³è¿‡
    
    key_offsets = idx * key_stride + tl.arange(0, D)
    value_offsets = idx * value_stride + tl.arange(0, D)
    key = tl.load(key_ptr + key_offsets)             # åŠ è½½ key
    value = tl.load(value_ptr + value_offsets)       # åŠ è½½ value
    
    cache_offsets = slot * D + tl.arange(0, D)
    tl.store(k_cache_ptr + cache_offsets, key)       # å­˜å‚¨åˆ° KV Cache
    tl.store(v_cache_ptr + cache_offsets, value)
```

### é€è¡Œè§£æ

| è¡Œå· | ä»£ç  | è¯´æ˜ |
|:---:|:---|:---|
| 1 | `@triton.jit` | Triton JIT ç¼–è¯‘è£…é¥°å™¨ |
| 3-8 | å‚æ•° | æŒ‡é’ˆå’Œæ­¥é•¿ï¼Œä»¥åŠå¸¸é‡ D |
| 10 | `program_id(0)` | è·å–çº¿ç¨‹ IDï¼ˆæ¯ä¸ª token ä¸€ä¸ªçº¿ç¨‹ï¼‰ |
| 11 | `tl.load(slot_mapping_ptr + idx)` | è¯»å–æ­¤ token åº”å­˜å‚¨çš„ slot |
| 12 | `if slot == -1: return` | CUDA Graph å¡«å……çš„æ— æ•ˆå€¼ |
| 14-17 | åŠ è½½ key/value | è®¡ç®—åç§»ï¼ŒåŠ è½½æ•°æ® |
| 19-21 | å­˜å‚¨ | å†™å…¥ KV Cache |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šä½¿ç”¨ Triton è€Œéçº¯ PyTorch å®ç° KV Cache å­˜å‚¨ï¼Œå› ä¸ºåˆ†æ•£å†™å…¥ï¼ˆ`slot_mapping` æŒ‡å®šä½ç½®ï¼‰åœ¨ PyTorch ä¸­éœ€è¦å¾ªç¯æˆ–å¤æ‚çš„ç´¢å¼•æ“ä½œã€‚Triton å¯ä»¥é€šè¿‡å¹¶è¡ŒåŒ–é«˜æ•ˆå®ç°ã€‚

### å†…å­˜å¸ƒå±€å›¾

```mermaid
graph LR
    subgraph "è¾“å…¥ Key [N, num_heads, head_dim]"
        K0["Token 0"]
        K1["Token 1"]
        K2["Token 2"]
    end
    
    subgraph "Slot Mapping"
        S0["slot 5"]
        S1["slot 12"]
        S2["slot 100"]
    end
    
    subgraph "KV Cache"
        C5["Slot 5"]
        C12["Slot 12"]
        C100["Slot 100"]
    end
    
    K0 -->|"idx=0"| S0 --> C5
    K1 -->|"idx=1"| S1 --> C12
    K2 -->|"idx=2"| S2 --> C100
```

---

## 10.4 store_kvcache åŒ…è£…å‡½æ•°

```python
def store_kvcache(key: torch.Tensor, value: torch.Tensor, k_cache: torch.Tensor, 
                  v_cache: torch.Tensor, slot_mapping: torch.Tensor):
    N, num_heads, head_dim = key.shape
    D = num_heads * head_dim
    
    # æ–­è¨€æ£€æŸ¥å†…å­˜å¸ƒå±€
    assert key.stride(-1) == 1 and value.stride(-1) == 1      # æœ€åç»´åº¦è¿ç»­
    assert key.stride(1) == head_dim and value.stride(1) == head_dim
    assert k_cache.stride(1) == D and v_cache.stride(1) == D
    assert slot_mapping.numel() == N
    
    # è°ƒç”¨ Triton kernel
    store_kvcache_kernel[(N,)](key, key.stride(0), value, value.stride(0), 
                               k_cache, v_cache, slot_mapping, D)
```

### å‚æ•°è¯´æ˜

| å‚æ•° | å½¢çŠ¶ | è¯´æ˜ |
|:---|:---|:---|
| `key` | `[N, num_heads, head_dim]` | å½“å‰ token çš„ Key |
| `value` | `[N, num_heads, head_dim]` | å½“å‰ token çš„ Value |
| `k_cache` | `[num_blocks, block_size, num_heads * head_dim]` | Key ç¼“å­˜ |
| `v_cache` | `[num_blocks, block_size, num_heads * head_dim]` | Value ç¼“å­˜ |
| `slot_mapping` | `[N]` | æ¯ä¸ª token çš„ç›®æ ‡ slot |

---

## 10.5 Attention ç±»

### 10.5.1 æ„é€ å‡½æ•°

```python
class Attention(nn.Module):

    def __init__(
        self,
        num_heads,
        head_dim,
        scale,
        num_kv_heads,
    ):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.scale = scale                              # 1 / sqrt(head_dim)
        self.num_kv_heads = num_kv_heads
        self.k_cache = self.v_cache = torch.tensor([])  # ç”± ModelRunner è®¾ç½®
```

### 10.5.2 å‰å‘ä¼ æ’­

```python
def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):
    context = get_context()
    k_cache, v_cache = self.k_cache, self.v_cache
    
    # å­˜å‚¨ KV Cache
    if k_cache.numel() and v_cache.numel():
        store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)
    
    if context.is_prefill:
        if context.block_tables is not None:    # prefix cache
            k, v = k_cache, v_cache             # ä½¿ç”¨ç¼“å­˜çš„ KV
        o = flash_attn_varlen_func(q, k, v,
                                   max_seqlen_q=context.max_seqlen_q, 
                                   cu_seqlens_q=context.cu_seqlens_q,
                                   max_seqlen_k=context.max_seqlen_k, 
                                   cu_seqlens_k=context.cu_seqlens_k,
                                   softmax_scale=self.scale, causal=True, 
                                   block_table=context.block_tables)
    else:    # decode
        o = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache,
                                    cache_seqlens=context.context_lens, 
                                    block_table=context.block_tables, 
                                    softmax_scale=self.scale, causal=True)
    return o
```

---

## 10.6 Prefill vs Decode å¯¹æ¯”

### 10.6.1 Prefill é˜¶æ®µ

```mermaid
graph TD
    subgraph "Prefill"
        Q["Q [total_tokens, num_heads, head_dim]"]
        K["K [total_tokens, num_kv_heads, head_dim]"]
        V["V [total_tokens, num_kv_heads, head_dim]"]
        FA["flash_attn_varlen_func"]
        O["Output"]
    end
    
    Q --> FA
    K --> FA
    V --> FA
    FA --> O
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | è¯´æ˜ |
|:---|:---|
| `cu_seqlens_q` | Q çš„ç´¯ç§¯åºåˆ—é•¿åº¦ `[0, 100, 200, 350]` |
| `cu_seqlens_k` | K çš„ç´¯ç§¯åºåˆ—é•¿åº¦ |
| `max_seqlen_q` | Q çš„æœ€å¤§åºåˆ—é•¿åº¦ |
| `max_seqlen_k` | K çš„æœ€å¤§åºåˆ—é•¿åº¦ |
| `block_table` | ç”¨äº Prefix Caching |

### 10.6.2 Decode é˜¶æ®µ

```mermaid
graph TD
    subgraph "Decode"
        Q["Q [batch, 1, num_heads, head_dim]"]
        KC["K Cache"]
        VC["V Cache"]
        FA["flash_attn_with_kvcache"]
        O["Output"]
    end
    
    Q --> FA
    KC --> FA
    VC --> FA
    FA --> O
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | è¯´æ˜ |
|:---|:---|
| `cache_seqlens` | æ¯ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡é•¿åº¦ |
| `block_table` | å—è¡¨ï¼ŒæŒ‡å‘ KV Cache ä¸­çš„ä½ç½® |

### 10.6.3 å¯¹æ¯”è¡¨

| ç‰¹æ€§ | Prefill | Decode |
|:---|:---|:---|
| Q å½¢çŠ¶ | `[total_tokens, h, d]` | `[batch, 1, h, d]` |
| K/V æ¥æº | å½“å‰è®¡ç®— or Cache | Cache |
| Flash Attention å‡½æ•° | `varlen_func` | `with_kvcache` |
| è®¡ç®—å¤æ‚åº¦ | O(nÂ²) | O(n) |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šPrefill å’Œ Decode ä½¿ç”¨ä¸åŒçš„ Flash Attention å‡½æ•°ï¼Œä½“ç°äº†ã€Œåˆ†è€Œæ²»ä¹‹ã€çš„ä¼˜åŒ–ç­–ç•¥ã€‚Prefill è¦å¤„ç†å®Œæ•´ä¸Šä¸‹æ–‡ï¼ŒDecode åªéœ€å¤„ç†æ–° token å’Œç¼“å­˜çš„ KVï¼Œä¸¤è€…è®¡ç®—æ¨¡å¼å®Œå…¨ä¸åŒã€‚

---

## 10.7 Prefix Caching å¤„ç†

```python
if context.is_prefill:
    if context.block_tables is not None:    # prefix cache
        k, v = k_cache, v_cache             # åˆ‡æ¢åˆ°ç¼“å­˜
```

### å¤„ç†é€»è¾‘

```mermaid
flowchart TD
    A{is_prefill?}
    A -->|å¦| B[Decode: ä½¿ç”¨ KV Cache]
    A -->|æ˜¯| C{æœ‰ Prefix Cache?}
    C -->|å¦| D[ä½¿ç”¨å½“å‰è®¡ç®—çš„ K, V]
    C -->|æ˜¯| E[ä½¿ç”¨ç¼“å­˜çš„ K, V + block_table]
    D --> F[flash_attn_varlen_func]
    E --> F
    B --> G[flash_attn_with_kvcache]
```

### Prefix Caching æ•ˆæœ

```
åºåˆ—: "ç³»ç»Ÿæç¤º(ç¼“å­˜)" + "ç”¨æˆ·é—®é¢˜(æ–°è®¡ç®—)"
      |---å·²ç¼“å­˜---|   |---éœ€è®¡ç®—---|
      
cu_seqlens_k > cu_seqlens_q  è¡¨ç¤ºæœ‰ç¼“å­˜
```

---

## 10.8 Flash Attention æ¥å£

### 10.8.1 flash_attn_varlen_func

```python
o = flash_attn_varlen_func(
    q, k, v,
    cu_seqlens_q=context.cu_seqlens_q,  # ç´¯ç§¯é•¿åº¦
    cu_seqlens_k=context.cu_seqlens_k,
    max_seqlen_q=context.max_seqlen_q,  # æœ€å¤§é•¿åº¦
    max_seqlen_k=context.max_seqlen_k,
    softmax_scale=self.scale,           # 1/sqrt(d)
    causal=True,                        # å› æœæ³¨æ„åŠ›
    block_table=context.block_tables    # å¯é€‰ï¼ŒPrefix Cache
)
```

### 10.8.2 flash_attn_with_kvcache

```python
o = flash_attn_with_kvcache(
    q.unsqueeze(1),                     # [batch, 1, h, d]
    k_cache, v_cache,                   # paged KV cache
    cache_seqlens=context.context_lens, # ä¸Šä¸‹æ–‡é•¿åº¦
    block_table=context.block_tables,   # å—è¡¨
    softmax_scale=self.scale,
    causal=True
)
```

---

## 10.9 å†…å­˜è®¿é—®ä¼˜åŒ–

### Triton Kernel ä¼˜åŒ–ç‚¹

1. **åˆå¹¶è®¿é—®**ï¼š`tl.arange(0, D)` ç”Ÿæˆè¿ç»­åç§»
2. **å¹¶è¡Œåº¦**ï¼šæ¯ä¸ª token ä¸€ä¸ªçº¿ç¨‹å—
3. **è·³è¿‡æ— æ•ˆ**ï¼š`slot == -1` ç›´æ¥è¿”å›

### Flash Attention ä¼˜åŒ–

1. **Tiling**ï¼šåˆ†å—è®¡ç®—ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
2. **Online Softmax**ï¼šæµå¼è®¡ç®—ï¼Œæ— éœ€å­˜å‚¨ä¸­é—´ç»“æœ
3. **Fused Kernel**ï¼šèåˆå¤šä¸ªæ“ä½œ

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šç›´æ¥å¼•ç”¨ Flash Attention åº“è€Œéè‡ªå·±å®ç°ï¼Œä½“ç°äº†ã€Œç«™åœ¨å·¨äººè‚©è†Šä¸Šã€çš„æ€æƒ³ã€‚Flash Attention æ˜¯æˆç†Ÿçš„é«˜æ€§èƒ½åº“ï¼Œå¤ç”¨å®ƒå¯ä»¥å¤§å¹…å‡å°‘å·¥ä½œé‡ã€‚

---

## 10.10 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **Triton Kernel**ï¼š
   - KV Cache å­˜å‚¨çš„é«˜æ•ˆå®ç°
   - slot_mapping çš„ä½œç”¨

2. **Attention ç±»**ï¼š
   - æ„é€ å‡½æ•°å‚æ•°
   - KV Cache ç»‘å®šæœºåˆ¶

3. **Prefill vs Decode**ï¼š
   - ä¸åŒçš„ Flash Attention å‡½æ•°
   - ä¸åŒçš„è¾“å…¥æ ¼å¼

4. **Prefix Caching**ï¼š
   - block_table çš„ä½¿ç”¨
   - cu_seqlens_k > cu_seqlens_q çš„åˆ¤æ–­

5. **Flash Attention æ¥å£**ï¼š
   - varlen_func å‚æ•°
   - with_kvcache å‚æ•°

---

**ä¸‹ä¸€ç« ** â†’ [11 RoPE ä½ç½®ç¼–ç ](11_rotary_embedding.md)
