# ç¬¬åä¸‰ç« ï¼šè¯åµŒå…¥ä¸è¾“å‡ºå¤´

> æœ¬ç« å°†é€è¡Œåˆ†æ `embed_head.py`ï¼Œç†è§£å¹¶è¡Œè¯åµŒå…¥å’Œè¯­è¨€æ¨¡å‹è¾“å‡ºå¤´çš„å®ç°ã€‚

## 13.1 å¹¶è¡Œè¯åµŒå…¥æ¦‚è¿°

è¯æ±‡è¡¨å¹¶è¡Œå°†è¯æ±‡è¡¨æŒ‰ GPU æ•°é‡åˆ‡åˆ†ï¼š

```mermaid
graph TB
    subgraph "è¯æ±‡è¡¨ (151936 è¯)"
        V0["è¯ 0-75967"]
        V1["è¯ 75968-151935"]
    end
    
    subgraph "GPU 0"
        E0["Embedding 0-75967"]
    end
    
    subgraph "GPU 1"
        E1["Embedding 75968-151935"]
    end
    
    V0 --> E0
    V1 --> E1
```

---

## 13.2 å®Œæ•´æºç 

```python
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist

from nanovllm.utils.context import get_context


class VocabParallelEmbedding(nn.Module):

    def __init__(
        self,
        num_embeddings: int,
        embedding_dim: int,
    ):
        super().__init__()
        self.tp_rank = dist.get_rank()
        self.tp_size = dist.get_world_size()
        assert num_embeddings % self.tp_size == 0
        self.num_embeddings = num_embeddings
        self.num_embeddings_per_partition = self.num_embeddings // self.tp_size
        self.vocab_start_idx = self.num_embeddings_per_partition * self.tp_rank
        self.vocab_end_idx = self.vocab_start_idx + self.num_embeddings_per_partition
        self.weight = nn.Parameter(torch.empty(self.num_embeddings_per_partition, embedding_dim))
        self.weight.weight_loader = self.weight_loader

    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
        param_data = param.data
        shard_size = param_data.size(0)
        start_idx = self.tp_rank * shard_size
        loaded_weight = loaded_weight.narrow(0, start_idx, shard_size)
        param_data.copy_(loaded_weight)

    def forward(self, x: torch.Tensor):
        if self.tp_size > 1:
            mask = (x >= self.vocab_start_idx) & (x < self.vocab_end_idx)
            x = mask * (x - self.vocab_start_idx)
        y = F.embedding(x, self.weight)
        if self.tp_size > 1:
            y = mask.unsqueeze(1) * y
            dist.all_reduce(y)
        return y


class ParallelLMHead(VocabParallelEmbedding):

    def __init__(
        self,
        num_embeddings: int,
        embedding_dim: int,
        bias: bool = False,
    ):
        assert not bias
        super().__init__(num_embeddings, embedding_dim)

    def forward(self, x: torch.Tensor):
        context = get_context()
        if context.is_prefill:
            last_indices = context.cu_seqlens_q[1:] - 1
            x = x[last_indices].contiguous()
        logits = F.linear(x, self.weight)
        if self.tp_size > 1:
            all_logits = [torch.empty_like(logits) for _ in range(self.tp_size)] if self.tp_rank == 0 else None
            dist.gather(logits, all_logits, 0)
            logits = torch.cat(all_logits, -1) if self.tp_rank == 0 else None
        return logits
```

---

## 13.3 VocabParallelEmbedding æ„é€ å‡½æ•°

```python
def __init__(
    self,
    num_embeddings: int,
    embedding_dim: int,
):
    super().__init__()
    self.tp_rank = dist.get_rank()                    # å½“å‰ GPU ID
    self.tp_size = dist.get_world_size()              # æ€» GPU æ•°
    assert num_embeddings % self.tp_size == 0         # ç¡®ä¿å¯å‡åˆ†
    
    self.num_embeddings = num_embeddings              # è¯æ±‡è¡¨å¤§å°
    self.num_embeddings_per_partition = self.num_embeddings // self.tp_size
    self.vocab_start_idx = self.num_embeddings_per_partition * self.tp_rank
    self.vocab_end_idx = self.vocab_start_idx + self.num_embeddings_per_partition
    
    self.weight = nn.Parameter(torch.empty(self.num_embeddings_per_partition, embedding_dim))
    self.weight.weight_loader = self.weight_loader
```

### è¯æ±‡è¡¨åˆ†åŒº

| GPU | start_idx | end_idx | è´Ÿè´£è¯æ±‡ |
|:---:|:---:|:---:|:---|
| 0 | 0 | 75968 | è¯ 0-75967 |
| 1 | 75968 | 151936 | è¯ 75968-151935 |

### å†…å­˜èŠ‚çœ

| é…ç½® | å• GPU å†…å­˜ | åŒ GPU å„è‡ª |
|:---|:---|:---|
| è¯æ±‡è¡¨ 151936, ç»´åº¦ 4096 | 2.3 GB | 1.15 GB |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šè¯æ±‡è¡¨å¹¶è¡Œæ˜¯å¤§è¯æ±‡è¡¨æ¨¡å‹çš„å¿…è¦ä¼˜åŒ–â€”â€”151K è¯æ±‡è¡¨çš„åµŒå…¥å±‚å ç”¨å¤§é‡å†…å­˜ã€‚åˆ†å‰²åæ¯ä¸ª GPU åªéœ€å­˜å‚¨éƒ¨åˆ†è¯æ±‡è¡¨ï¼Œå¤§å¹…å‡å°‘æ˜¾å­˜å‹åŠ›ã€‚

---

## 13.4 weight_loader æ–¹æ³•

```python
def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor):
    param_data = param.data
    shard_size = param_data.size(0)                   # æœ¬åˆ†åŒºå¤§å°
    start_idx = self.tp_rank * shard_size             # èµ·å§‹ä½ç½®
    loaded_weight = loaded_weight.narrow(0, start_idx, shard_size)  # åˆ‡ç‰‡
    param_data.copy_(loaded_weight)
```

**åˆ‡ç‰‡ç¤ºæ„**ï¼š

```mermaid
graph LR
    subgraph "åŸå§‹æƒé‡"
        A["#91;151936, 4096#93;"]
    end
    
    subgraph "GPU 0"
        B["narrow(0, 0, 75968)"]
        C["#91;75968, 4096#93;"]
    end
    
    subgraph "GPU 1"
        D["narrow(0, 75968, 75968)"]
        E["#91;75968, 4096#93;"]
    end
    
    A --> B --> C
    A --> D --> E
```

---

## 13.5 VocabParallelEmbedding å‰å‘ä¼ æ’­

```python
def forward(self, x: torch.Tensor):
    if self.tp_size > 1:
        # åˆ›å»ºæ©ç ï¼šåªå¤„ç†æœ¬ GPU è´Ÿè´£çš„è¯
        mask = (x >= self.vocab_start_idx) & (x < self.vocab_end_idx)
        # å°†è¯ ID è½¬æ¢ä¸ºæœ¬åœ°ç´¢å¼•
        x = mask * (x - self.vocab_start_idx)
    
    y = F.embedding(x, self.weight)
    
    if self.tp_size > 1:
        # éæœ¬ GPU è´Ÿè´£çš„è¯ï¼Œè¾“å‡ºç½®é›¶
        y = mask.unsqueeze(1) * y
        # AllReduce åˆå¹¶ç»“æœ
        dist.all_reduce(y)
    
    return y
```

### é€è¡Œè§£æ

| è¡Œå· | ä»£ç  | è¯´æ˜ |
|:---:|:---|:---|
| 3 | `mask = ...` | æ ‡è®°å“ªäº›è¯ç”±æœ¬ GPU å¤„ç† |
| 4 | `x = mask * (x - start)` | è½¬æ¢ä¸ºæœ¬åœ°ç´¢å¼•ï¼Œä¸åœ¨èŒƒå›´å†…çš„å˜ä¸º 0 |
| 6 | `F.embedding(x, self.weight)` | æŸ¥è¡¨è·å–åµŒå…¥ |
| 8 | `mask.unsqueeze(1) * y` | ä¸åœ¨èŒƒå›´å†…çš„åµŒå…¥ç½®é›¶ |
| 9 | `all_reduce(y)` | æ±‚å’Œåˆå¹¶å„ GPU ç»“æœ |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šä½¿ç”¨ã€Œæ©ç  + ç½®é›¶ + AllReduceã€å¤„ç†è·¨ GPU æŸ¥è¯¢ï¼Œè™½ç„¶çœ‹èµ·æ¥æœ‰å†—ä½™è®¡ç®—ï¼Œä½†é¿å…äº†å¤æ‚çš„è·¯ç”±é€»è¾‘ã€‚æ¯ä¸ª GPU è®¡ç®—æ‰€æœ‰ tokenï¼Œä½†åªæœ‰è´Ÿè´£èŒƒå›´å†…çš„éé›¶ï¼Œç®€åŒ–äº†å®ç°ã€‚

### æ‰§è¡Œç¤ºä¾‹

```
è¾“å…¥: x = [0, 80000, 50000, 100000]
è¯æ±‡è¡¨: 151936, åŒ GPU

GPU 0 (è´Ÿè´£ 0-75967):
  mask = [True, False, True, False]
  local_x = [0, 0, 50000, 0]
  y[0] = embedding[0], y[2] = embedding[50000]
  y[1] = y[3] = 0

GPU 1 (è´Ÿè´£ 75968-151935):
  mask = [False, True, False, True]
  local_x = [0, 4032, 0, 24032]  # 80000-75968=4032
  y[1] = embedding[4032], y[3] = embedding[24032]
  y[0] = y[2] = 0

AllReduce å: æ­£ç¡®çš„åµŒå…¥å‘é‡
```

### é€šä¿¡ç¤ºæ„

```mermaid
sequenceDiagram
    participant GPU0
    participant GPU1
    
    Note over GPU0,GPU1: è¾“å…¥ x = [0, 80000]
    
    GPU0->>GPU0: embedding[0] (æœ‰æ•ˆ)
    GPU0->>GPU0: 0 å‘é‡ (80000 ä¸åœ¨èŒƒå›´)
    
    GPU1->>GPU1: 0 å‘é‡ (0 ä¸åœ¨èŒƒå›´)
    GPU1->>GPU1: embedding[4032] (æœ‰æ•ˆ)
    
    GPU0->>GPU1: AllReduce
    GPU1->>GPU0: AllReduce
    
    Note over GPU0,GPU1: ç»“æœ = [embedding[0], embedding[80000]]
```

---

## 13.6 ParallelLMHead ç±»

```python
class ParallelLMHead(VocabParallelEmbedding):

    def __init__(
        self,
        num_embeddings: int,
        embedding_dim: int,
        bias: bool = False,
    ):
        assert not bias
        super().__init__(num_embeddings, embedding_dim)
```

ç»§æ‰¿ VocabParallelEmbeddingï¼Œå…±äº«æƒé‡åŠ è½½é€»è¾‘ã€‚

---

## 13.7 ParallelLMHead å‰å‘ä¼ æ’­

```python
def forward(self, x: torch.Tensor):
    context = get_context()
    
    # Prefill æ—¶åªå–æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ª token
    if context.is_prefill:
        last_indices = context.cu_seqlens_q[1:] - 1
        x = x[last_indices].contiguous()
    
    # çº¿æ€§å˜æ¢å¾—åˆ° logits
    logits = F.linear(x, self.weight)
    
    if self.tp_size > 1:
        # Gather åˆ° Rank 0
        all_logits = [torch.empty_like(logits) for _ in range(self.tp_size)] if self.tp_rank == 0 else None
        dist.gather(logits, all_logits, 0)
        logits = torch.cat(all_logits, -1) if self.tp_rank == 0 else None
    
    return logits
```

### é€è¡Œè§£æ

| è¡Œå· | ä»£ç  | è¯´æ˜ |
|:---:|:---|:---|
| 4-6 | `last_indices` | Prefill åªéœ€æ¯ä¸ªåºåˆ—æœ€åä½ç½®çš„ logits |
| 8 | `F.linear(x, self.weight)` | è®¡ç®— logits = x @ W^T |
| 10-13 | `dist.gather` | æ”¶é›†å„ GPU çš„ logits åˆ° Rank 0 |
| 14 | `torch.cat` | æ‹¼æ¥æˆå®Œæ•´è¯æ±‡è¡¨çš„ logits |

### å–æœ€å token

```
cu_seqlens_q = [0, 100, 200, 350]
last_indices = [99, 199, 349]  # æ¯ä¸ªåºåˆ—çš„æœ€åä½ç½®

hidden_states: [350, hidden_dim]
    â†“ å–æœ€å token
extracted: [3, hidden_dim]  # 3 ä¸ªåºåˆ—
```

### Gather vs AllReduce

```mermaid
graph TB
    subgraph "AllReduce (åµŒå…¥å±‚)"
        A1["GPU 0: éƒ¨åˆ†ç»“æœ"] --> B1["æ‰€æœ‰ GPU å¾—åˆ°å®Œæ•´ç»“æœ"]
        A2["GPU 1: éƒ¨åˆ†ç»“æœ"] --> B1
    end
    
    subgraph "Gather (è¾“å‡ºå¤´)"
        C1["GPU 0: éƒ¨åˆ† logits"] --> D1["Rank 0 å¾—åˆ°å®Œæ•´ logits"]
        C2["GPU 1: éƒ¨åˆ† logits"] --> D1
        E["å…¶ä»– GPU: None"]
    end
```

**ä¸ºä»€ä¹ˆè¾“å‡ºå¤´ç”¨ Gatherï¼Ÿ**

- åªæœ‰ Rank 0 éœ€è¦æ‰§è¡Œé‡‡æ ·
- èŠ‚çœé€šä¿¡é‡ï¼ˆä¸éœ€è¦å¹¿æ’­ç»“æœï¼‰

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šè¾“å…¥ç«¯ç”¨ AllReduceï¼ˆæ‰€æœ‰ GPU éœ€è¦åµŒå…¥ç»“æœï¼‰ï¼Œè¾“å‡ºç«¯ç”¨ Gatherï¼ˆåªæœ‰ Rank 0 é‡‡æ ·ï¼‰ã€‚è¿™ç§ã€ŒæŒ‰éœ€é€šä¿¡ã€çš„è®¾è®¡æœ€å¤§é™åº¦å‡å°‘äº†è·¨ GPU æ•°æ®ä¼ è¾“ã€‚

---

## 13.8 æƒé‡å…±äº«

åœ¨ Qwen3 æ¨¡å‹ä¸­ï¼ŒåµŒå…¥å±‚å’Œè¾“å‡ºå¤´å¯ä»¥å…±äº«æƒé‡ï¼š

```python
# qwen3.py
if config.tie_word_embeddings:
    self.lm_head.weight.data = self.model.embed_tokens.weight.data
```

**ä¼˜åŠ¿**ï¼š

| é…ç½® | å‚æ•°é‡ | å†…å­˜ |
|:---|:---|:---|
| ç‹¬ç«‹æƒé‡ | 2 Ã— è¯æ±‡è¡¨ Ã— ç»´åº¦ | ~4.6 GB |
| å…±äº«æƒé‡ | 1 Ã— è¯æ±‡è¡¨ Ã— ç»´åº¦ | ~2.3 GB |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šæƒé‡å…±äº«æ˜¯ç°ä»£ LLM çš„æ ‡å‡†åšæ³•â€”â€”åµŒå…¥å±‚å­¦ä¹ ã€Œè¯â†’å‘é‡ã€æ˜ å°„ï¼Œè¾“å‡ºå¤´å­¦ä¹ ã€Œå‘é‡â†’è¯ã€æ˜ å°„ï¼Œä¸¤è€…é€»è¾‘ä¸Šæ˜¯å¯¹ç§°çš„ï¼Œå…±äº«æƒé‡åœ¨æ•°å­¦ä¸Šåˆç†ä¸”èŠ‚çœä¸€åŠå†…å­˜ã€‚

---

## 13.9 logits å½¢çŠ¶å˜åŒ–

### Prefill é˜¶æ®µ

```
è¾“å…¥: hidden_states [total_tokens, hidden_dim]
å–æœ€å: [num_seqs, hidden_dim]
è¾“å‡º: logits [num_seqs, vocab_size]
```

### Decode é˜¶æ®µ

```
è¾“å…¥: hidden_states [batch, hidden_dim]
è¾“å‡º: logits [batch, vocab_size]
```

---

## 13.10 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **VocabParallelEmbedding**ï¼š
   - è¯æ±‡è¡¨åˆ†åŒºç­–ç•¥
   - æ©ç æœºåˆ¶å¤„ç†è·¨ GPU æŸ¥è¯¢
   - AllReduce åˆå¹¶ç»“æœ

2. **ParallelLMHead**ï¼š
   - ç»§æ‰¿åµŒå…¥å±‚è®¾è®¡
   - Prefill æ—¶å–æœ€å token
   - Gather æ”¶é›† logits åˆ° Rank 0

3. **é€šä¿¡ç­–ç•¥**ï¼š
   - åµŒå…¥å±‚ï¼šAllReduceï¼ˆæ‰€æœ‰ GPU éœ€è¦ç»“æœï¼‰
   - è¾“å‡ºå¤´ï¼šGatherï¼ˆåªæœ‰ Rank 0 é‡‡æ ·ï¼‰

4. **æƒé‡å…±äº«**ï¼š
   - åµŒå…¥å±‚å’Œè¾“å‡ºå¤´å¯å…±äº«æƒé‡
   - å‡å°‘å†…å­˜å ç”¨

---

**ä¸‹ä¸€ç« ** â†’ [14 é‡‡æ ·å™¨](14_sampler.md)
