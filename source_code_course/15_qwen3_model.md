# ç¬¬åäº”ç« ï¼šQwen3 æ¨¡å‹å®ç°

> æœ¬ç« å°†é€è¡Œåˆ†æ `qwen3.py`ï¼Œç†è§£å®Œæ•´çš„ Transformer æ¨¡å‹å¦‚ä½•ç»„è£…å„å±‚ç»„ä»¶ã€‚

## 15.1 æ¨¡å‹æ¶æ„æ¦‚è¿°

```mermaid
graph TB
    subgraph "Qwen3ForCausalLM"
        A[Qwen3Model]
        B[ParallelLMHead]
    end
    
    subgraph "Qwen3Model"
        C[VocabParallelEmbedding]
        D["Qwen3DecoderLayer Ã— N"]
        E[RMSNorm]
    end
    
    subgraph "Qwen3DecoderLayer"
        F[Input LayerNorm]
        G[Qwen3Attention]
        H[Post-Attn LayerNorm]
        I[Qwen3MLP]
    end
    
    A --> C
    A --> D
    A --> E
    D --> F
    D --> G
    D --> H
    D --> I
```

---

## 15.2 Qwen3Attention

### 15.2.1 æºç 

```python
class Qwen3Attention(nn.Module):

    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int,
        max_position: int = 4096 * 32,
        head_dim: int | None = None,
        rms_norm_eps: float = 1e-06,
        qkv_bias: bool = False,
        rope_theta: float = 10000,
        rope_scaling: tuple | None = None,
    ) -> None:
        super().__init__()
        tp_size = dist.get_world_size()
        self.total_num_heads = num_heads
        assert self.total_num_heads % tp_size == 0
        self.num_heads = self.total_num_heads // tp_size
        self.total_num_kv_heads = num_kv_heads
        assert self.total_num_kv_heads % tp_size == 0
        self.num_kv_heads = self.total_num_kv_heads // tp_size
        self.head_dim = head_dim or hidden_size // self.total_num_heads
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        self.scaling = self.head_dim ** -0.5
        self.qkv_bias = qkv_bias

        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=qkv_bias,
        )
        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=False,
        )
        self.rotary_emb = get_rope(
            self.head_dim,
            rotary_dim=self.head_dim,
            max_position=max_position,
            base=rope_theta,
            rope_scaling=rope_scaling,
        )
        self.attn = Attention(
            self.num_heads,
            self.head_dim,
            self.scaling,
            self.num_kv_heads,
        )
        if not self.qkv_bias:
            self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
            self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
```

### 15.2.2 æ„é€ å‡½æ•°åˆ†æ

**å¼ é‡å¹¶è¡Œé…ç½®**ï¼š

```python
tp_size = dist.get_world_size()
self.num_heads = self.total_num_heads // tp_size      # æ¯ GPU çš„ Q å¤´æ•°
self.num_kv_heads = self.total_num_kv_heads // tp_size  # æ¯ GPU çš„ KV å¤´æ•°
```

**æŠ•å½±å±‚**ï¼š

| å±‚ | ç±»å‹ | è¯´æ˜ |
|:---|:---|:---|
| `qkv_proj` | QKVParallelLinear | åˆå¹¶çš„ QKV æŠ•å½± |
| `o_proj` | RowParallelLinear | è¾“å‡ºæŠ•å½± + AllReduce |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šQKV åˆå¹¶ä¸ºä¸€ä¸ªçº¿æ€§å±‚å‡å°‘äº†å†…æ ¸å¯åŠ¨å¼€é”€ã€‚æ³¨æ„ `qkv_proj` ä½¿ç”¨åˆ—å¹¶è¡Œæ— éœ€é€šä¿¡ï¼Œè€Œ `o_proj` ä½¿ç”¨è¡Œå¹¶è¡Œéœ€è¦ AllReduceï¼Œæ•´ä½“æ¯å±‚åªæœ‰ä¸€æ¬¡é€šä¿¡ã€‚

**QKV å½’ä¸€åŒ–**ï¼ˆQwen3 ç‰¹æœ‰ï¼‰ï¼š

```python
if not self.qkv_bias:
    self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
    self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
```

### 15.2.3 å‰å‘ä¼ æ’­

```python
def forward(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
) -> torch.Tensor:
    qkv = self.qkv_proj(hidden_states)
    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
    q = q.view(-1, self.num_heads, self.head_dim)
    k = k.view(-1, self.num_kv_heads, self.head_dim)
    v = v.view(-1, self.num_kv_heads, self.head_dim)
    if not self.qkv_bias:
        q = self.q_norm(q)
        k = self.k_norm(k)
    q, k = self.rotary_emb(positions, q, k)
    o = self.attn(q, k, v)
    output = self.o_proj(o.flatten(1, -1))
    return output
```

**æ•°æ®æµ**ï¼š

```mermaid
graph LR
    A[hidden_states] --> B[qkv_proj]
    B --> C[split Q,K,V]
    C --> D[reshape]
    D --> E[Q/K Norm]
    E --> F[RoPE]
    F --> G[Attention]
    G --> H[flatten]
    H --> I[o_proj]
    I --> J[output]
```

---

## 15.3 Qwen3MLP

```python
class Qwen3MLP(nn.Module):

    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
    ) -> None:
        super().__init__()
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,
            bias=False,
        )
        self.down_proj = RowParallelLinear(
            intermediate_size,
            hidden_size,
            bias=False,
        )
        assert hidden_act == "silu"
        self.act_fn = SiluAndMul()

    def forward(self, x):
        gate_up = self.gate_up_proj(x)
        x = self.act_fn(gate_up)
        x = self.down_proj(x)
        return x
```

### MLP ç»“æ„

```mermaid
graph LR
    A[è¾“å…¥ x] --> B[gate_up_proj]
    B --> C["#91;gate, up#93;"]
    C --> D["SiLU(gate) Ã— up"]
    D --> E[down_proj]
    E --> F[è¾“å‡º]
```

**å‚æ•°é‡**ï¼š

| å±‚ | å½¢çŠ¶ | å‚æ•°é‡ |
|:---|:---|:---|
| gate_up_proj | [hidden, 2Ã—intermediate] | h Ã— 2i |
| down_proj | [intermediate, hidden] | i Ã— h |

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šSwiGLU ç»“æ„ï¼ˆ`gate Ã— SiLU(up)`ï¼‰æ¯”ä¼ ç»Ÿ FFN è¡¨è¾¾èƒ½åŠ›æ›´å¼ºã€‚å°† gate å’Œ up åˆå¹¶ä¸ºä¸€ä¸ªçº¿æ€§å±‚ç„¶åæ‹†åˆ†ï¼Œæ¯”ä¸¤ä¸ªç‹¬ç«‹çš„çº¿æ€§å±‚æ›´é«˜æ•ˆã€‚

---

## 15.4 Qwen3DecoderLayer

```python
class Qwen3DecoderLayer(nn.Module):

    def __init__(
        self,
        config: Qwen3Config,
    ) -> None:
        super().__init__()
        self.self_attn = Qwen3Attention(
            hidden_size=config.hidden_size,
            num_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads,
            max_position=config.max_position_embeddings,
            rms_norm_eps=config.rms_norm_eps,
            qkv_bias=getattr(config, 'attention_bias', True),
            head_dim=getattr(config, 'head_dim', None),
            rope_theta=getattr(config, "rope_theta", 1000000),
            rope_scaling=getattr(config, "rope_scaling", None),
        )
        self.mlp = Qwen3MLP(
            hidden_size=config.hidden_size,
            intermediate_size=config.intermediate_size,
            hidden_act=config.hidden_act,
        )
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        residual: torch.Tensor | None,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        if residual is None:
            hidden_states, residual = self.input_layernorm(hidden_states), hidden_states
        else:
            hidden_states, residual = self.input_layernorm(hidden_states, residual)
        hidden_states = self.self_attn(positions, hidden_states)
        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
        hidden_states = self.mlp(hidden_states)
        return hidden_states, residual
```

### æ®‹å·®è¿æ¥æµ

```mermaid
graph TB
    subgraph "DecoderLayer"
        A[è¾“å…¥] --> B{ç¬¬ä¸€å±‚?}
        B -->|æ˜¯| C[LN + ä¿å­˜æ®‹å·®]
        B -->|å¦| D[æ®‹å·®åŠ æ³• + LN]
        C --> E[Self-Attention]
        D --> E
        E --> F[Post-LN + æ›´æ–°æ®‹å·®]
        F --> G[MLP]
        G --> H[è¾“å‡º + æ®‹å·®]
    end
```

---

## 15.5 Qwen3Model

```python
class Qwen3Model(nn.Module):

    def __init__(
        self,
        config: Qwen3Config,
    ) -> None:
        super().__init__()
        self.embed_tokens = VocabParallelEmbedding(config.vocab_size, config.hidden_size)
        self.layers = nn.ModuleList([Qwen3DecoderLayer(config) for _ in range(config.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
    ) -> torch.Tensor:
        hidden_states = self.embed_tokens(input_ids)
        residual = None
        for layer in self.layers:
            hidden_states, residual = layer(positions, hidden_states, residual)
        hidden_states, _ = self.norm(hidden_states, residual)
        return hidden_states
```

### å‰å‘ä¼ æ’­æµç¨‹

```mermaid
graph TB
    A[input_ids] --> B[embed_tokens]
    B --> C["Layer 0"]
    C --> D["Layer 1"]
    D --> E["..."]
    E --> F["Layer N-1"]
    F --> G[Final Norm]
    G --> H[hidden_states]
    
    subgraph "æ®‹å·®æµ"
        R[residual] -.-> C
        C -.-> R1[æ›´æ–°]
        R1 -.-> D
    end
```

---

## 15.6 Qwen3ForCausalLM

```python
class Qwen3ForCausalLM(nn.Module):
    packed_modules_mapping = {
        "q_proj": ("qkv_proj", "q"),
        "k_proj": ("qkv_proj", "k"),
        "v_proj": ("qkv_proj", "v"),
        "gate_proj": ("gate_up_proj", 0),
        "up_proj": ("gate_up_proj", 1),
    }

    def __init__(
        self,
        config: Qwen3Config
    ) -> None:
        super().__init__()
        self.model = Qwen3Model(config)
        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)
        if config.tie_word_embeddings:
            self.lm_head.weight.data = self.model.embed_tokens.weight.data

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
    ) -> torch.Tensor:
        return self.model(input_ids, positions)

    def compute_logits(
        self,
        hidden_states: torch.Tensor,
    ) -> torch.Tensor:
        return self.lm_head(hidden_states)
```

### 15.6.1 packed_modules_mapping

```python
packed_modules_mapping = {
    "q_proj": ("qkv_proj", "q"),
    "k_proj": ("qkv_proj", "k"),
    "v_proj": ("qkv_proj", "v"),
    "gate_proj": ("gate_up_proj", 0),
    "up_proj": ("gate_up_proj", 1),
}
```

**ä½œç”¨**ï¼šæƒé‡æ˜ å°„ï¼Œå°† HuggingFace æ ¼å¼è½¬æ¢ä¸ºåˆå¹¶æ ¼å¼ã€‚

| åŸå§‹æƒé‡ | ç›®æ ‡æƒé‡ | shard_id |
|:---|:---|:---|
| `model.layers.0.self_attn.q_proj.weight` | `model.layers.0.self_attn.qkv_proj.weight` | "q" |
| `model.layers.0.mlp.gate_proj.weight` | `model.layers.0.mlp.gate_up_proj.weight` | 0 |

### 15.6.2 æƒé‡å…±äº«

```python
if config.tie_word_embeddings:
    self.lm_head.weight.data = self.model.embed_tokens.weight.data
```

åµŒå…¥å±‚å’Œè¾“å‡ºå¤´å…±äº«æƒé‡ï¼Œå‡å°‘å‚æ•°é‡ã€‚

### 15.6.3 åˆ†ç¦»çš„ forward å’Œ compute_logits

```python
def forward(self, input_ids, positions):
    return self.model(input_ids, positions)

def compute_logits(self, hidden_states):
    return self.lm_head(hidden_states)
```

**åŸå› **ï¼šCUDA Graph åªæ•è· `forward`ï¼Œé‡‡æ ·åœ¨ `compute_logits` åæ‰§è¡Œã€‚

> ğŸ’¡ **è®¾è®¡æ€æƒ³**ï¼šåˆ†ç¦» `forward` å’Œ `compute_logits` æ˜¯ä¸ºäº† CUDA Graph ä¼˜åŒ–â€”â€”Graph åªæ•è·æ¨¡å‹å‰å‘ä¼ æ’­ï¼Œè€Œ logits è®¡ç®—åœ¨ Graph å¤–æ‰§è¡Œã€‚è¿™æ ·å¯ä»¥çµæ´»å¤„ç†ä¸åŒæ‰¹æ¬¡å¤§å°çš„è¾“å‡ºã€‚

---

## 15.7 å®Œæ•´æ¶æ„å›¾

```mermaid
graph TB
    subgraph "Qwen3ForCausalLM"
        subgraph "Qwen3Model"
            A[VocabParallelEmbedding]
            
            subgraph "Layers"
                subgraph "Layer 0"
                    B1[Input LayerNorm]
                    subgraph "Qwen3Attention"
                        C1[QKVParallelLinear]
                        D1[Q/K Norm]
                        E1[RoPE]
                        F1[Attention]
                        G1[RowParallelLinear]
                    end
                    H1[Post-Attn LayerNorm]
                    subgraph "Qwen3MLP"
                        I1[MergedColumnParallel]
                        J1[SiLUÃ—Mul]
                        K1[RowParallel]
                    end
                end
                L["... (N layers)"]
            end
            
            M[Final RMSNorm]
        end
        
        N[ParallelLMHead]
    end
    
    A --> B1
    B1 --> C1 --> D1 --> E1 --> F1 --> G1
    G1 --> H1 --> I1 --> J1 --> K1
    K1 --> L --> M --> N
```

---

## 15.8 Qwen3 é…ç½®ç¤ºä¾‹

```python
Qwen3Config(
    vocab_size=151936,
    hidden_size=4096,
    intermediate_size=11008,
    num_hidden_layers=28,
    num_attention_heads=32,
    num_key_value_heads=8,  # GQA
    hidden_act="silu",
    max_position_embeddings=4096,
    rms_norm_eps=1e-6,
    attention_bias=False,  # ä½¿ç”¨ Q/K Norm
    tie_word_embeddings=True,
)
```

---

## 15.9 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **Qwen3Attention**ï¼š
   - QKV åˆå¹¶æŠ•å½±
   - Q/K å½’ä¸€åŒ–ï¼ˆQwen3 ç‰¹æœ‰ï¼‰
   - RoPE ä½ç½®ç¼–ç 

2. **Qwen3MLP**ï¼š
   - Gate-Up åˆå¹¶æŠ•å½±
   - SiLU é—¨æ§æ¿€æ´»

3. **Qwen3DecoderLayer**ï¼š
   - æ®‹å·®è¿æ¥æµ
   - Pre-LN æ¶æ„

4. **Qwen3Model**ï¼š
   - åµŒå…¥ + å¤šå±‚ Decoder + Final Norm

5. **Qwen3ForCausalLM**ï¼š
   - packed_modules_mapping
   - æƒé‡å…±äº«
   - forward ä¸ compute_logits åˆ†ç¦»

---

**ä¸‹ä¸€ç« ** â†’ [16 å·¥å…·æ¨¡å—](16_utils.md)
